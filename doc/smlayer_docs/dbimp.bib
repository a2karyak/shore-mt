@unpublished{ObjectStore,
	Author = "{Object Design, Inc.}",
	Title = "{ObjectStore} User's Guide",
	Address = "Burlington, MA",
	Year = 1992}

@unpublished{Ontos,
	Author = "{Ontos, Inc.}",
	Title = "{Ontos} Reference Manual",
	Address = "Burlington, MA",
	Year = 1992}

@unpublished{Objectivity,
	Author = "{Objectivity, Inc.}",
	Title = "{Objectivity} Reference Manual",
	Address = "Menlo Park, CA",
	Year = 1992}

@unpublished{Versant,
	Author = "{Versant, Inc.}",
	Title = "{Versant} Reference Manual",
	Address = "Menlo Park, CA",
	Year = 1992}

@inproceedings{as:ano,
	Author = "Paul Adams and Marvin Solomon",
	Title = "An Overview of the {CAPITL} Software Development Environment",
	Booktitle = "Proceedings of the Fourth International Workshop on
                     Software Configuration Management",
	Address = "Baltimore, MD",
	Year = 1993}

@article{bgmp:the,
	Author = "M. W. Blasgen and J. Gray and M. Mitoma and T. Price",
	Title = "The Convoy Phenomenon",
	Journal = "Operating System Review",
	Volume = 13,
	Number = 2,
	Year = 1979}

@article{be:sto,
	Author = "M. W. Blasgen and K. P. Eswaran",
	Title = "Storage and access in relational databases",
	Journal = "IBM Systems Journal",
	Volume = 16,
	Number = 4,
	Year = 1977,
	Annote = "Apparently advocates sort-merge over nested-loops
                  with index in almost all cases unless the probed relation
                  is clustered on the indexed join attribute."}

@inproceedings{egks:abeproc,
	Author = "S. Englert and J. Gray and T. Kocher and P. Shah",
	Title = "A benchmark of {NonStop} {SQL} {Release 2} Demonstrating
		Near-Linear Speedup and Scaleup on Large Database",
	Booktitle = "Proceedings of the SIGMETRICS Conference",
	Year = 1990,
	Month = "May",
	Pages = "245-247"}


@techreport{egks:abe,
	Author = "S. Englert and J. Gray and T. Kocher and P. Shah",
	Title = "A benchmark of NonStop SQL Release 2 Demonstrating
		Near-Linear Speedup and Scaleup on Large Database",
	Institution = "Tandem Computers",	
	Number = "89.4, Tandem Part No. 27469",
	Year = 1989}

@techreport{t:thi,
	Author = "{The Committee for Advanced DBMS Function}",
	Title = "Third-Generation Data Base System Manifesto",
	Institution = "University of California, Berkeley",
	Number = "UCB/ERL M90/28",
	Month = "April",
	Year = "1990",
	Annote = "The authors propose and discuss 3 basic tenets and 13
                  propositions for DBMS of the 90's.  They are:
		\begin{enumerate}
		\item Besides traditional data management services, third
                      generation DBMSs will provide support for richer
                      object structures and rules.
		\item Third Generation DBMSs must subsume second generation
                      DBMSs.
		\item Third generation DBMSs must be open to other subsystems.
		\end{enumerate}
		with propositions
		\begin{enumerate}
		\item A third generation DBMS must have a rich type system.
		\item Inheritance is a good idea.
		\item Functions, including database procedures and methods,
		      and encapsulation are a good idea.
		\item UIDs for records should be assigned by the DBMS only
                      if a user-defined primary key is not available.
		\item Rules (triggers, constraints) will become a major feature
		      in future systems.  They should not be associated with a
                      specific function or collection.
		\item Essentially all programatic access to the database should
                      be through a non-procedural, high-level access language.
		\item There should be at least two ways to specify collections,
		      one using enumeration of members and one using the query
                      language to specify membership.
		\item Updatable views are essential.
		\item Performance indicators have almost nothing to do with
		      data models and must not appear in them.
		\item Third generation DBMSs must be accessible from multiple
                      HLLs.
		\item Persistent X for a variety of Xs is a good idea.  They
		      will all be supported on top of a single DBMS by 
                      compiler extensions and a complex runtime system.
		\item For better or worse, SQL is intergalactic dataspeak.
		\item Queries and their resulting answers should be the
                      lowest level of communication between a client and a
                      server.
		\end{enumerate}"}

@unpublished{e:usi,
	Author = "The {EXODUS Group}",
	Title = "Using the {EXODUS} Storage Manager {V2.0.2}",
	Institution = "Department of Computer Sciences, University of Wisconsin",
	Month = "January",
	Year = 1992,
	Note = "Technical Documentation"}
	
	

@inproceedings{hflp:ext,
	Author = "L. M. Haas and J. C. Freytag and G. M. Lohman and 
                  H. Pirahesh",
	Title = "Extensible Query Processing in {Starburst}",
	Booktitle = "Proceedings of the 1989 ACM-SIGMOD 
		     Conference on the Management of Data",
	Address = "Portland, OR",
	Month = "June",
	Year = 1989,
	Pages = "377-388"}

@book{rv:con,
	Author = "A. W. Roberts and D. E. Varberg",
	Title = "Convex Functions",
	Publisher = "Academic Press",
	Address = "New York, N.Y.",
	Year = 1973}

@book{r:con,
	Author = "R. T. Rockafellar",
	Title = "Convex Analysis",
	Publisher = "Princeton University Press",
	Address = "Princeton, NJ",
	Year = "1970"}

@misc{f:per,
	Author = "M. Ferris",
	Note = "Personal communication",
	Month = "November",
	Year =  1989}

@misc{d:per,
	Author = "R. DeLeone",
	Note = "Personal communication",
	Month = "November",
	Year =  1989}



@misc{s:pc,
	Author = "P. Selinger",
	Note = "Personal communication",
	Month = "June",
	Year =  1989}

@book{m:int,
	Author = "R. E. Moore",
	Title = "Interval Analysis",
	Publisher = "Prentice Hall",
	Address = "Englewood Cliffs, NJ",
	Year = 1966}

@article{rh:und,
	Title = "Understanding and Extending Transformation-Based Optimizers",
	Author = "A. Rosenthal and  P. Helman",
	Journal = "Data Engineering",
	Volume = 9,
	Number = 4,
	Month  = "December",
	Year = 1986}

@book{rr:com,
	Author = "H. Ratschek and J. Rokne",
	Title = "Computer Methods for the Range of Functions",
	Publisher = "Ellis Horwood Ltd.",
	Address = "West Sussex, England",
	Year = 1984}


@article{jvv:ran,
	Author = "M. R. Jerrum and L. G. Valiant and V. V. Vazirani",
	Title = "Random Generation of Combinatorial Structures from a 
		 Uniform Distribution",
	Journal = "Theoretical Computer Science",
	Volume = 43,
	Year = 1986,
	Pages = "169--188"}

@book{vt:cal,
	Author = "C. G. {Van der Laan} and N. M. Temme",
	Title = "Calculation of special functions: the 
		gamma function, the exponential integrals and 
		error-like functions",
	Publisher = "CWI",
	Address = "Amsterdam, The Netherlands",
	Year = 1984}

@book{s:mat,
	Author = "B. M. Shchigolev",
	Title = "Mathematical Analysis of Observations",
	Publisher = "American Elsevier",
	Address = "New York, NY",
	Year = "1965"}


@article{sj:app,
	Author = "A. Sinclair and M. Jerrum",
	Title = "Approximate Counting, Uniform Generation and 
		 Rapidly Mixing Markov Chains",
	Journal = "Information and Computation",
	Volume = 82,
	Year = "1989",
	Pages = "93--133"}

@techreport{bktj:rep,
	Author = "H. E. Bal and M. F. Kaashoek and A. S. Tanenbaum and
                  J. Jansen",
	Title = "Replication Techniques for Speeding up Parallel 
                 Applications on Distributed Systems",
	Institution = "Department of Mathematics and Computer Science,
		       Vrije Universiteit",
	Number = "IR-202",
	Month = "October",
	Year = 1989,
	Annote = "Advocates shared objects, rather than shared memory
                  pages, and considers two implementations: based on
                  writes invalidate copies, and writes are propagated
                  to copies.  The conclusion is that which is better
                  depends upon the underlying support."}
	

@inproceedings{bbkv:fad,
	Author= "F. Bancilhon and T. Briggs and S. Khoshafian and P. Valduriez",
	Title = "{FAD}, a Powerful and Simple Database Language",
	Booktitle = "Proc. VLDB Conf.",
	Address = "Brighton, England",
	Year = 1987}

@article{b:dat,
	Author = "J. {Banerjee et al.}",
	Title = "Data Model Issues for Object Oriented Applications",
	Journal = "ACM Transactions on Office Information Systems",
	Volume = 5,
	Number = 1,
	Pages = "3--26",
	Month = "January",
	Year = 1987}

@article{bkkg:clu,
	Author = "Jay Banerjee and Won Kim and Sung-Jo Kim and Jorge F. Garza",
	Title = "Clustering a {DAG} for {CAD} Databases",
	Journal = "IEEE Transactions on Software Engineering",
	Volume = 14,
	Number = 11,
	Month = "November",
	Year = 1988,
	Pages = "1684 --1699",
	Annote = "Says that if you are doing DFS on a DAG, then
		DFS clustering is a good idea.  Also proposes
		that you use B-tree like mechanisms: keep each
		page between 1/2 and completely full, merge on
		too empty, redistribute if can't merge, etc."}

@techreport{bgp:apr,
	Author = "Daniel Barbara and Hector Garcia-Molina and Daryl Porter",
	Title = "A Probabilistic Relational Data Model",
	Institution = "Princeton University",
	Number = "CS-TR-215-89",
	Month = "January",
	Year = 1989,
	Annote = "The key attributes of the model are:
		1) It allows uncertain probabilities, e.g., X is true
                with 0.4 probability, Y with 0.2, and * with 0.4.
		2) It contains projects (which compute conditional
                probabilities) and selects (e.g., all tuples in which
                X {must be, could be} true with 0.5 probability) and
                joins, which compute probabilities of combinations."}

@inproceedings{bg:par,
	Author = "{Bj\o rn Arild W.} Baugst\o\ and {Jarle Fredrik} Greipsland",
	Title = "Parallel Sorting Methods for Large Data on a Hypercube
		 Database Computer",
	Booktitle = "Proceedings of the Sixth International Workshop
		 on Database Machines",
	Year = 1989,
	Month = "June",
	Address = "Deauville, France",
	Publisher = "Springer-Verlag",
	Pages = "127 -- 141",
	Annote = "Basic idea: local sort, then systematic sampling
		of 2\% of the sorted tuples, then partition, then
		tournament sort.  Does not discuss how to arrive
		at 2\%.  Presents analytic and experimental
		results for the Norwegian Institute of Technology
		database computer."}

@article{bhjlc:dis,
	Author = "Andrew Black and Norman Hutchinson and
		Erick Jul and Henry Levy and Larry Carter",
	Title = "Distribution and Abstract Types in Emerald",
	Journal = "IEEE Transactions on Software Engineering",
	Volume = "SE-13",
	Number = 1,
	Month = "January",
	Year = 1987,
	Pages = "65 -- 76",
	Annote = "Emerald is a statically typed object-oriented programming
		language designed for building distributed systems and
		applications.  There is a single object model for all
		objects --- each object consists of an ID, a representation,
		a set of operations, and possibly a process.  Abstract
		types are defined by signatures (essentially templates)
		and there is a formal notion of conformance for 
		compatibility.  This conformance essentially means that
		if A conforms to B, an A can be used wherever a B can
		be used.  One subtlety is that this requires that
		the arguments to the operations of the two must
		conform in the reverse direction --- for a shared method,
		B's argument types must be able to be used anywhere A's
		can."}

@incollection{bfks:joi,
	Author = "C. Baru and O. Frieder and D. Kandlur and M. Segal",
	Title = "Join on a Cube: Analysis, Simulation, and Implementation",
	Booktitle = "Database Machines and Knowledge Base Machines",
	Editor = "M. Kitsuregawa and H. Tanaka",
	Publisher = "Kluwer Academic Publishers",
	Year = 1987}

@article{bbw:sor,
	Author = "Micah Beck and Dina Bitton and W. Kevin Wilkinson",
	Title = "Sorting Large Files on a Backend Multiprocessor",
	Journal = "IEEE Transactions on Computers",
	Volume = 37,
	Number = 7,
	Year = 1988,
	Pages = "769 -- 778",
	Annote = "Discussiong of parallel merge sort for both
		single input-single output and multiple input-
		single output sorts."}

@inproceedings{blmpsz:aco,
	Author = "G. E. Blelloch and C. E. Leiserson and B. M. Maggs and
		C. G. Plaxton and S. J. Smith and M. Zagha",
	Title = "A comparison of sorting algorithms for the
		{C}onnection {M}achine {CM}-2",
	Booktitle = "Proceedings of the 3rd Annual ACM Symposium on Parallel
		Algorithms and Architectures", 
	Address = "Hilton Head, North Carolina",
	Month = "July",
	Year = 1991,
	Note = "To appear."}

@inproceedings{bbg:ame,
	Author = "A. Borg and J. Baumbach and S. Glazer",
	Title = "A message system supporting fault tolerance",
	Booktitle = "Proceedings of the ACM Symposium on Operating System 
	Principles",
	Address = "Atlanta, Georgia", 
	Month = "October",
	Year = 1983,
	Pages = "90--99"}

@article{bs:awe,
	Author = "Bruno Braban and Peter Schlenk",
	Title = "A Well Structured Parallel File System for {PM}",
	Journal = "SIGOPS Newsletter",
	Year = 1989,
	Pages = "25--38",
	Annote = "Describes an 'everything is an object' file system.
                  Supports transactions, locks on abitrary subparts of
                  a file, and so forth.  The goal is to allow concurrency
                  while still guaranteeing transaction semantics and 
                  data consistency."}

@incollection{b:alg,
	Title = "Algebra Operations on a Parallel Computer -- 
	         Performance Evaluation",
	Author = "Kjell Bratbergsengen",
	Booktitle = "Database Machines and Knowledge Base Machines",
	Editor = "M. Kitsuregawa and H. Tanaka",
	Publisher = "Kluwer Academic Publishers",
	Year = 1987}

@book{bd:mat,
	Author = "Peter J. Bickel and Kjell A. Doksum",
	Title = "Mathematical Statistics: Basic Ideas and Selected Topics",
	Publisher = "Holden-Day, Inc.",
	Address = "San Fransisco, California",
	Year = 1977}

@article{bos:the,
	Author = "Paul Butterworth and Allen Otis and Jacob Stein",
	Title = "The {GemStone} Object Database Management System",
	Journal = "Communications of the ACM",
	Volume = 34,
	Number = 10,
	Month = "October",
	Year = 1991}


@unpublished{cfz:fin,
	Author = "M. Carey and M. Franklin and M. Zaharioudakis", 
	Title = "Fine-grained Sharing in a Page-Server {OODBMS}", 
	Month = "December",
	year = 1993,
	Note = "Submitted for publication."}

@inproceedings{cdn:the,
	Author = "Michael J. Carey and 
                  David J. DeWitt and Jeffrey F. Naughton",
	Title = "The {OO7} Benchmark",
	Booktitle = "Proceedings of the 1993 ACM-SIGMOD Conference
                     on the Management of Data",
	Month = "May",
	Year = 1993,
	Address = "Washington D.C."}

@incollection{cdrs:sto,
	Author = "M. Carey and D. DeWitt and J. Richardson and E. Shekita",
	Title = "Storage Management for Object in {EXODUS}",
	Booktitle = "Object-Oriented Concepts, Databases, and Applications",
	Editors = "W. Kim and F. Lochovsky",
	Publisher = "Addison-Wesley",
	Year = 1989}

@inproceedings{cdv:ada,
	Author = "M. J. Carey and D. J. DeWitt and S. L. Vandenberg",
	Title = "A Data Model and Query Language for EXODUS",
	Booktitle = "Proceedings of the 1988 ACM-SIGMOD Conference 
		     on the Management of Data",
	Address = "Chicago, Illinois",
	Pages = "413-423",
	Month = "June",
	Year = 1988}

@article{cl:distributed,
	Author = "{K. Mani} Chandy and Leslie Lamport",
	Title = "Distributed Snapshots: Determining Global States
		of Distributed Systems",
	Journal = "ACM Transactions on Computer Systems",
	Volume = 3,
	Number = 1,
	Month = "February",
	Year = 1985,
	Pages = "63--75",
	Annote = "Basic idea: to take a checkpoint, each processor
		sends a special marker on all outgoing edges.  
		After taking a checkpoint, log all incoming messages
		on each incoming edge until a special marker arrives
		(special case: if marker arrives before checkpoint,
		log zero messages.)  Intuitively, this means that
		the states of the processes are in their individual
		checkpoints, and the states of the communication links
		capture differentials in when these checkpoints were
		actually taken."}

@inproceedings{cm:uni,
	Author = "D. R. Cheriton and T. P. Mann",
	Title = "Uniform access to distributed name interpretation
                 in the {V}-System",
	Booktitle = "Proceedings of the 4th IEEE International Conference
                     on Distributed Computing Systems",
	Month = "May",
	Year = 1984}

@article{cacbm:per,
	Author = "W. P. Cockshot and M. P. Atkinson and K. J. Chisholm
                  and P. J. Bailey and R. Morrison",
	Title = "Persistent Object Managment System",
	Journal = "Software Practice and Experience",
	Volume = "",
	Number = "",
	Year = 1984,
	Pages = "",
	Annote = "Describes an object store to support PS-Algol"}


@inproceedings{d:adb,
	Author = "P. {Dadam et al.}",
	Title = "A {DBMS} Prototype to Support Extended 
		{${\rm NF}^2$} Relations: An Integrated 
		View on Flat Tables and Hierarchies",
	Booktitle = "Proceedings of the 1986 ACM-SIGMOD Conference 
		     on the Management of Data",
	Address = "Washington, DC",
	Pages = "356-367",
	Month = "May",
	Year =  1986}


@article{b:gen,
	Author = "D. {Batory et al}.",
	Title = "GENESIS: An Extensible Database Management System",
	Journal = "IEEE Transactions on Software Engineering",
	Volume =  14,
	Number =  11,
	Month = "November",
	Year = 1988,
	Pages = "1711--1730"}

@article{hnr:afo,
	Author = "P. E. Hart and N. J. Nilsson and B. Raphael",
	Title = "A formal basis for the heuristic determination of
		 minimum cost paths",
	Journal = "IEEE Transactions on System Science and Cybernetics",
	Volume = "SSC-4",
	Number = 2,
	Pages = "100--107",
	Year = 1968}


@article{yl:ani,
	Author = "H. Yoo and S. Lafortune",
	Title = "An Intelligent Search Method for Query Optimization 
		by Semijoins",
	Journal = "IEEE Transactions on Knowledge and Data Engineering",
	Volume = 1,
	Number = 2,
	Month = "June",
	Year =  "1989",
	Pages = "226--237"}



@misc{h:per,
	Author = "Don Haderle",
	Month = "December",
	Year = 1989,
	Note = "Personal Communication."}

@inproceedings{ag:sch88,
	Author = "Robert Abbot and Hector Garcia-Molina",
	Title = "Scheduling Real-time Transactions: a Performance
		 Evaluation",
	Booktitle = "Proceedings of the Fourteenth 
		     International Conference on Very Large Databases",
	Year = 1988,
	Month = "August",
	Address = "Los Angeles, California",
	Pages = "1--12",
	Annote = "The authors identify the following problems in the
		  design and evaluation of a RTDBS: What is the best
		  data model?  What languages can be used to express
                  real-time constraints?  What mechanisms are needed
		  for describing and evaluating triggers?  How are
		  transactions scheduled?  How do real-time constraints
                  affect concurrency control?  This paper is a first cut
                  at the last two.  The difference between RTDBS and real
                  time systems on the one hand and DBMS on the other is
                  the combination of concurrency control and realtime
                  deadlines.  The options considered here are describable
		  by three dimensions:
		  \begin{itemize}
			\item Eligibility:
				\begin{itemize}
					\item All Eligible
					\item Not Tardy
					\item Feasible Deadlines
				\end{itemize}
			\item Priority:
				\begin{itemize}
					\item First Come First Serve
					\item Earliest Deadline
					\item Least Slack
				\end{itemize}
			\item Concurrency:
				\begin{itemize}
					\item Serial Execution
					\item High Priority
					\item Conditional Restart
				\end{itemize}
		  \end{itemize}
		  Overall they found that Earliest Deadline with
                  Feasible Deadlines and Conditional Restart
                  performed best."} 

@inproceedings{ag:sch89,
	Author = "Robert Abbot and Hector Garcia-Molina",
	Title = "Scheduling Real-time Transactions with Disk Resident
		 Data",
	Booktitle = "Proceedings of the Fifteenth 
		     International Conference on Very Large Databases",
	Year = 1989,
	Month = "August",
	Address = "Amsterdam, The Netherlands",
	Pages = "385--396",
	Annote = "They studies three ways to assign priorities:
		  First-come First-Serve, Earliest Deadline,
		  and Least Slack.  Also, four ways of resolving
		  conflicts on locked data: Wait, Wait-promote,
		  High-priority, Conditional Restart.  Finally,
		  two ways of doing I/O scheduling: FIFO, and Priority.

		  Their conclusions, based on simulations under a
		  high-load scenario, are that Least Slack is
		  the best overall, Wait Promote is the best overall,
		  and that when IO system is heavily loaded, priority
		  is much better than FCFS.

		  In many cases the differences are only within a
		  few percent."}  


@inproceedings{abbgrty:mac,
	Author = "Mike Accetta and Robert Baron and William Bolosky
	          and David Golub and Richard Rashid and Avadis Tevanian 
		  and Michael Young",
	Title = "Mach: a new kernel foundation for {UNIX} development",
	Booktitle = "Proceedings of the Summer USENIX Conference",
	Year = 1986,
	Address = "Atlanta, Georgia",
	Pages = "93--112",
	Annote = "Overview of Mach."}

@article{ac:the,
	Author = "Rakesh Agrawal and Michael Carey",
	Title = "The Performance of Concurrency Control and
	         Recovery Algorithms for Transaction-Oriented
		 Database Systems",
	Journal = "Bulletin of the IEEE Committee on Database
	           Engineering",
        Volume = 8,
	Number = 2,
	Month = "June",
	Pages = "58--67",
        Year = 1985}

@article{acl:con,
        Author = "R. Agrawal and M. Carey and M. Livny",
	Title = "Concurrency Control Performance Modeling:
                 Alternatives and Implications",
	Journal = "ACM Transactions on Database Systems",
	Volume = 12,
	Number = 4,
	Month = "December",
	Year = 1987}

@inproceedings{ad:rec,
	Author = "Rakesh Agrawal and David J. DeWitt",
	Title = "Recovery Architectures for Multiprocessor Database
	         Machines",
	Booktitle = "Proceedings of the ACM SIGMOD International
	             Conference on Management of Data",
	Year = 1985,
	Pages = "131--145",
	Annote = "Surveys three approaches: parallel logging,
	          shadow paging, and differential files.  Conclusion
		  is that parallel logging is best, and that
		  only one log processor is necessary ``because
		  disk bandwidth slows query processors to the
		  point that more log processors cannot be
		  kept busy."}

@inproceedings{ac:pro,
	Author = "William Alexander and George Copeland",
	Title = "Process and Dataflow Control in Distributed
	         Data-Intensive Systems",
	Booktitle = "Proceedings of the SIGMOD International
		     Conference on Management of Data",
	Year = 1988,
	Pages = "90--98",
	Address = "Chicago, Illinois",
	Month = "May",
	Annote = "In Data-Intensive systems, operations and 
	          intermediate results move instead of base 
		  data.  This paper surveys some of the 
		  problems that arise.  I couldn't find
		  the point of this paper."}

@techreport{abln:the,
	Author = "Guy T. Almes and Andrew P. Black and
		  Edward D. Lazowska and Jerre D. Noe",
	Title = "The Eden System: A Technical Overview",
	Institution = "Department of Computer Science,
		       University of Washington",
	Year = 1983,
	Month = "October",
	Number = "83-10-05",
	Annote = "This paper describes the design of Eden,
	          some aspects of the implementation (how
		  things are divided between th kernel, the
		  programming language, and library code),
		  and an outline of how some applications
		  use the system."}

@inproceedings{a:the,
	Author = "T. {Anderson et al.}",
	Title = "The {HyperModel} {Benchmark}",
	Booktitle = "Proceedings of the EDBT Conference",
	Address = "Venice, Italy",
	Month = "March",
	Year = 1990} 

@article{a:ame,
	Author = "Anon",
	Title = "A Measure of Transaction Processing Power",
	Journal = "Datamation",
	Volume = 31,
	Number = 7,
	Month = "April",
	Year = 1985,
	Pages = "112--118",
	Annote = "Measures cost/performance for various systems on
	          three benchmarks: Debit-credit, Scan, and Sort."}

@inproceedings{ael:rea,
	Author = "Andrew W. Appel and John R. Ellis and Kai Li",
	Title = "Real-time concurrent garbage collection on stock 
	         multiprocessors",
	Booktitle = "ACM SIGPLAN Conference on Programming Language Design
		     and Implementation",
	Year = 1988}

@inproceedings{ab:ane,
	Author = "James Archibald and Jean-Loup Baer",
	Title = "An Economical Solution to the Cache Coherence Problem",
	Booktitle = "Proceedings of the Eleventh Annual International
		     Symposium on Computer Architecture",
	Month = "June",
	Year = 1984,
	Address = "Ann Arbor",
	Pages = "355--362",
	Annote = "Proposes a solution to coherent caches in non-shared
	          bus multiprocessors.  Contains a good introduction
		  to the issues."}
	

@article{ab:cac,
	Author = "James Archibald and Jean-Loup Baer",
	Title = "Cache Coherence Protocols: Evaluation Using
                 a Multiprocessor Simulation Model",
	Journal = "ACM Transactions on Computer Systems",
	Month = "November",
	Year = 1986,
	Volume = 4,
	Number = 4,
	Pages = "273--298",
	Annote = "A good survey of Cache coherence protocols.  Concludes
          Write-once improved classical by precluding one invalidation per
          write, Berkeley improved Write-once by requiring only one write 
	  if written multiple times (instead of two.),  Illinois 
	  improved Berkely by detecting sharing, hence precluding
	  invalidation of writes to unshared data, Firefly improved 
	  Illinois by precluding invalidation by having caches 
	  overwrite instead of invalidating."}

@manual{a:iPS,
	Author = "Ramune Arlauskas",
	Title = "{iPSC/2 System}: A Second Generation Hypercube",
	Organization = "Intel Corporation",
	Pages = "9--13",
	Year = "1988"}


@techreport{asw:cou,
	Author = "M. Astrahan and M. Schkolnick and K. Whang",
	Title = "Counting unique values of an attribute without
		 sorting",
	Institution = "IBM Research Division",
	Number = "RJ 4960",
	Year = 1985}

@article{aea:sys,
	Author = "M. M. Astrahan and M. W. Blasgen and D. D.
		  Chamberlin and K. P. Eswaran and J. N. Gray
		  and P. G. Griffiths and W. F. King and R. A.
		  Lorie and P. R. McJones and J. W. Mehl and
		  G. R. Putzolu and I. L. Traiger and B. W. 
		  Wade and V. Watson",
        Title = "System {R}: A Relational Approach to Data Management",
	Journal = "ACM Transactions on Database Systems",
	Month = "June",
	Year = 1976,
	Pages = "97--137",
	Volume = 1,
	Number = 2}

@article{as:mul,
	Author = "William C. Athas and Charles L. Seitz",
	Title = "Multicomputers: Message-Passing Concurrent
	         Computers",
	Journal = "IEEE Computer",
	Month = "August",
	Year = 1988,
	Pages = "9--24",
	Annote = "Good study of ``Multicomputers,'' which are
                  essentially hypercube-like machines."}

@article{ab:typ,
	Author = "Malcom P. Atkinson and O. Peter Buneman",
	Title = "Types and Persistence in Database Programming
		 Languages",
	Journal = "ACM Computing Surveys",
	Month = "June",
	Year = 1987,
	Volume = 19,
	Number = 2,
	Pages = "105--190",
	Annote = "Don't have it --- too long to copy."}

@techreport{a:801,
	Author = "C. R. Attanasio",
	Title = "801 Architecture Support for Database --- a Case
	         Study",
	Institution = "Advanced Minicomputer Systems, IBM T. J. Watson
		       Research Center",
	Year = 1987,
	Month = "January",
	Number = "RC 12416",
	Annote = "The 801 system provides a 48 bit virtual address
		  space, and level 3 consistency for segments in
		  this virtual address space.  A subset of SQL,
		  SQL/801, has been implemented on this storage.
		  CPR is the control program implementing this
		  storage.  CPR provides 4096 segments of up to
		  256 Mbytes.  Segments are mapped into virtual
		  address spaces programs.  CPR uses a 2-phase lock
		  policy.  The program using a segment uses explicit
		  open, commit, and rollback operations.
		  
		  Makes heavy use of the System/38 type ``inverted
		  pages table'' so that page table size is 
		  proportional to real, not virtual, storage.
		  
		  SQL/801 uses one segment per table, index, or view.
		  CPR locks at 128 byte resolution.

		  Advantages of virtual store: no separate DB
		  buffering; buffer management and consistency
		  are intergrated in OS; no moving of data
		  to working store."}

@inproceedings{b:liv,
	Author = "Robert M. Balzer",
	Title = "Living in the Next Generation Operating System",
	Booktitle = "Proceedings of the 10th IFIP Congress",
	Month = "September",
	Year = 1986,
	Address = "Dublin, Ireland",
	Annote = "The first generation OS used files as intermodule
		  communication.  The second used pipes, allowing
		  a limited amount of feedback.  This (second and current)
		  generation suffers from limitations on control and data
		  representation.  The next generation is what he
		  calls the extended dbms model.  Communication is
		  through shared, structured, persistent data.
		  The main point is that communication is through
                  the extended database, where each participant
                  in the communication declares what objects of
                  that database and which circumstances it is
                  interested in."}

@article{bbh:con,
	Author = "J. Banerjee and R. I. Baum and D. K. Hsiao",
	Title = "Concepts and Capabilities of a Database Computer",
	Journal = "ACM Transactions on Database Systems",
	Volume = 3,
	Number = 4,
	Month = "December",
	Year = 1978}

@inproceedings{b:ano,
	Author = "Joel Bartlett",
	Title = "A {NonStop} Kernel",
	Booktitle = "Proceedings of the 8th Symposium on Operating Systems 
                     Principles",
	Month = "December",
	Year = 1981,
	Pages = "22-29",
	Annote = "The main interesting thing is that all processes
	          exist in pairs, the main process, and the backup.
		  Operations are performed in such a way that both
		  would have to fail if the operation is to fail,
		  and since the processes run on separate hardware
		  this is unlikely.  The reliability is achieved
		  through a protocol based on sequence numbers and
		  saved results."}

@inproceedings{bcphp:out,
        Author = "J. L. Bergerand and P. Caspi and D. Pilaud and
                  N. Halbwachs and E. Pilaud",
        Title = "Outline of a Real Time Data Flow Language",
        Booktitle = "Proceedings of the IEEE Real-Time Systems
                     Symposium",
        Year = 1985,
        Month = "December",
        Address = "San Diego, California",
        Pages = "33--42",
        Annote = "The authors describe LUSTRE, an extension of
                  LUCID for real-time.  The authors are very
                  concerned about a clean denotational semantics.
                  The central notion is that since variables are
                  streams in LUCID, why not let the nth value
                  of x represent the value of x at time n?
                  Additionally, they add some operators to help
                  with combining streams, selecting subsets of
                  streams, and so forth."}

@inproceedigs{bmr:est,
	Author = "Gerard Berry and Sabine Moisan and Jean-Paul Rigault",
	Title = "{ESTEREL}: Towards a synchronous and semantically
		 sound high level language for Real Time applications",
	Booktitle = "Proceedings of the IEEE Real-Time Systems Symposium",
	Year = 1983,
	Month = "December",
	Address = "Arlington, Virginia",
	Pages = "30--37",
	Annote = "ESTEREL has several real-time related features.  First,
		  in its semantics, things happen immediately and
		  simultaneously (e.g., in x;y, there is no time between
		  the end of x and the start of y.)  Second, there is a
		  do in parallel construct; third, there is a way of
		  signalling an event, EMIT <event> [ (value) ].  Events
		  can be hardware interrupts as well.  An interesting
		  aspect of the language is that there is no unique
		  real-time clock --- the only time is that marked by
		  events (which could be an interval timer timeout.)

		  Perhaps the most interesting thing is the single 
		  temporal construct:

	  	  upto <event-occurrence> [(<variable name>)] 
			do <statement1>
			abnormal <statement2> end.

		  The intuitive semantics is simple: immediately
		  start executing statement 1.  There are two ways
		  to terminate: if statement1 completes, the upto
		  completes; if not, and the event occurs, then
		  statement2 is executed.

		  Event occurrence can be 
			n>>e
		  which means after the nth occurence of event e.
			
		  A lot of other constructs can be built out of
		  this one: e.g.
			upto e1 do a end,
			wait e1,
			from e1 to e2 do a end
			during n*e1 do a end
			delay n*e1
			on e1 do a end
			every e1 do a end"}



@article{bbdw:par,
	Author = "Dina Bitton and Haran Boral and David J. Dewitt 
		  and W. Kevin Wilkinson",
	Title = "Parallel Algorithms for the Execution of Relational
		 Database Operations",
	Journal = "ACM Transactions on Database Systems",
	Volume = 8,
	Number = 3,
	Month = "September",
	Year = 1983,
	Pages = "324--353",
	Annote = "Describes some algorithms for joins, projects,
	          sorting, and selection on parallel machines.
		  First, assumes that there are no indices in 
		  the relations (!).  Also, assumes that
		  processor's memories are small, but that there
		  are lots of them, but that still not all
		  of the tuples in all relations will fit in
		  the memories at once.  So, the analysis 
		  is heavily dependent on I/O cost, and is
		  in terms of blocks.  Communication is through
		  a shared disk cache.
		  
		  The conclusion is that the ``best''
		  algorithm depends on such things as
		  relative relation sizes and that
		  the asymptotically best algorithm may not
		  really be very good"}

@article{bd:dup,
	Author = "Dina Bitton and David J. Dewitt",
	Title = "Duplicate Record Elimination in Large Data Files",
	Journal = "ACM Transactions on Database Systems",
	Volume = 8,
	Number = 2,
	Month = "June",
	Year = 1983,
	Pages = "255--265",
	Annote = "Compares several ways of eliminating duplicates.
		  The ``standard'' method is an external merge sort 
		  followed by a scan.  The authors show that it is
		  faster if you eliminate duplicates as you find them
		  during the merge.  The speedup is related to
		  the fraction of the file that is duplicates."}

@article{bdhm:ata,
	Author = "Dina Bitton and David J. DeWitt and David K. Hsiao
                  and Jaishankar Menon",
	Title = "A Taxonomy of Parallel Sorting",
	Journal = "Computing Surveys",
	Volume = 16,
	Number = 3,
	Month = "September",
	Year = 1984,
	Pages = "287--318",
	Annote = "Good bibliography for sorting, and good overviews
                  of some broad classes of sorting algorithms.  Gives
                  the impression that almost nothing was known about
                  the performance of external sorting algorithms at
                  the time it was written --- might still be true."}
	

@inproceedings{bdt:ben,
	Author = "Dina Bitton and David J. DeWitt and
		  Carolyn Turbyfill",
	Title = "Benchmarking Database Systems: A Systematic Approach",
	Booktitle = "Proceedings of the Ninth International Conference
		     on Very large data bases",
	Year = 1983,
	Pages = "8--19",
	Annote = "Describes a set of benchmarks run on RTI Ingres,
		  University Ingres, and the IDM-5000.  The
		  methodology was to construct relations of
		  tuples of integers, where each field contained
		  random integers between 1 and n (n = size of 
		  relation produces a key, n = 1 produces all
		  identical values.)  Then they ran joins,
		  selects, and projects of various projectivities.
		  
		  The point is hard to discern - it seems to be
		  that you have to be careful about using indices
		  and choosing join algorithms."}

@inproceedings{bz:iss,
	Author = "Toby Bloom and Stanley B. Zdonik",
	Title = "Issues in the Design of Object-Oriented Database
	         Programming Languages",
	Booktitle = "OOPSLA Proceedings",
	Year = 1987,
	Pages = "441--451",
	Annote = ""}

@inproceedings{bd:ame,
	Author = "Haran Boral and David J. Dewitt",
	Title = "A Methodology for Database System Performance
	         Evaluation",
	Booktitle = "Proceedings of the ACM SIGMOD International
	             Conference on Management of Data",
        Year = 1984,
	Month = "June",
	Address = "Boston, Massachusetts",
	Pages = "176--185",
	Annote = "States that the three factors that affect
	          database system performance in multiuser
		  environments are multiprogramming level,
		  degree of data sharing, and query mix.
		  They argue that simple metrics like
		  elapsed time, CPU usage, and amount
		  of I/O activity are best, and that the
		  system should be tested by benchmarks
		  on synthetic data."}

@inproceedings{br:dat,
	Author = "Haran Boral and Steve Redfield",
	Title = "Database Machine Morphology",
	Booktitle = "Proceedings of the Eleventh International Conference
		     on Very Large Databases",
	Month = "August",
	Year = 1985,
	Pages = "59--71",
	Annote = "Presents a classification scheme for DB machines
		  and classifies about 20 machines.  Conclusions are
		  that we need more work on the I/O bottleneck,
		  specialized DB OS, and throughput vs. turnaround."}

@article{cjr:cho,
	Author = "Roy Campbell and Garry Johnston and Vincent Russo",
	Title = "{CHOICES} ({C}lass {H}ierarchical {O}pen {I}nterface for
	                   {C}ustom {E}mbedded {S}ystems)",
	Journal = "Operating Systems Review",
	Volume = 21,
	Number = 3,
	Month = "July",
	Year = 1987,
	Pages = "9--18",
	Annote = "Choices is an object oriented family of operating
	          systems, customizable for applications and 
		  hardware.  It uses shared virtual memory for
    		  interprocess communication, and supports 
		  shared persistent objects.  (This paper
		  goes into very little detail on these
		  issues.)  Currently a version runs on a
		  10 processor multimax."}


@inproceedings{cdfgmrs:the,
	Author = "Michael J. Carey and David J. Dewitt and 
		  Daniel Frank and Goetz Graefe and
		  M. Muralikrishna and Joel E. Richardson and
		  Eugene J. Shekita",
	Title = "The Architecture of the {EXODUS} {Extensible} {DBMS}",
	Booktitle = "Proceedings of the Twelfth International
		     Conference on Very Large Data Bases",
	Year = 1986,
	Pages = "52--65",
	Annote = "Exodus is intended to simplify the development
		 of high-performance, application-specific DBMS.
		 It consists of the Storage Object Manager
		 and the Type Manager.  The Type Manager is
		 an extensible class-based schema management
		 system.  In addition EXODUS provides E, 
		 essentially persistent C, for building
		 DBMS.  EXODUS also includes a rule-based
		 optimizer --- all queries must first be
		 converted to a tree, and valid
		 transformations on the tree must be given.
		 (Presumably a cost function as well.)"}

@inproceedings{cdrs:obj,
	Author = "Michael J. Carey and David J. Dewitt and Joel E.
		  Richardson and Eugene J. Shekita",
	Title = "Object and File Management in the {EXODUS} Extensible
	         Database System",
	Booktitle = "Proceedings of the Twelfth International 
		     Conference on Very Large Data Bases",
	Year = 1986,
	Pages = "91--100",
	Annote = "EXODUS provides low-level routines to manage
		  storage, providing hooks for access methods
		  and concurrency control and recovery.  The
		  system supports two kinds of objects, large
		  and small.  Small objects fit on a disk
		  page and are identified by their disk addresses;
   		  large objects span multiple pages and are
		  represented as B-Tree indices on byte
		  arrays.  The focus is on supporting efficient
		  lookup and updating of ranges of bytes
		  within objects.  There are a couple of
		  tricks used to try to maintain an acceptable
		  level of disk usage."}

@book{c:ods,
	Author = "R. Cattell",
	Title = "The {Object Database Standard}: {ODMG}-93",
	Publisher = "Morgan Kaufmann",
	Address = "San Mateo, CA",
	Year = 1993}

@article{cs:obj,
	Author = "R. Cattell and J. Skeen",
	Title = "Object Operations Benchmark",
	Journal = "ACM Transactions on Database Systems",
	Volume = 17,
	Number = 1,
	Month = "March",
	Year = 1992}

@inproceedings{cyw:sch,
	Author = "Ming-Syan Chen and Philip S. Yu and Kun-Lung Wu",
	Title = "Scheduling and Processor Allocation for the Execution
                 of Multi-Join Queries in a Multiprocessor System",
	Booktitle = "Proceedings of the International Conference on
	             Data Engineering",
	Year = 1992,
	Note = "To appear.",
	Annote = "Consists of two main parts: 

	(1) An evaluation of join ordering heuristics, for both
	sequential and bushy trees.
	(2) Some heuristics for processor allocation.  The choices considered
	are 
		(a) Sequential execution, 
		Allocate all processors to every join, one join at a time.

		(b) Fixed size, 
		Allocate a fixed number of processors to each join (to avoid
		fragmentation.)

		(c) Minimum time,
		For each join allocate the number of processors that minimizes
		the time for that join.

		(d) Time-efficiency.
		For each join allocate the number of processors that minimizes
		the time*numProc product for that join.

	Furthermore, they consider two ways to schedule, top-down (all
	processors to the root, allocate for each level below so as to
	have all children of a node finishing at about the same time) or
	bottom-up (just begin allocating as determined by the heuristic
	being used while satisfying precedence constraints.)  This gives

		(e) CT_{SE} (Coordinated time using serial execution.)
		(d) CT_{TE} (Coordinated time using time efficiency.)

	The conclusions (from a rather bizarre set of random queries
	and cost functions) are that CT_{SE} is best.  That is, build
	a good uniprocessor busy tree, then use coordinated execution time
	to schedule it."} 

@inproceedings{ck:rea,
        Author = "Albert N. Copper III and John P. Kearns",
        Title  = "Real-Time Distributed Control with Asynchronous
                  Message Reception",
        Booktitle = "Proceedings of the IEEE Real-Time Systems
                     Symposium",
        Year = 1985,
        Month = "December",
        Address = "San Diego, California",
        Pages = "67--75",
        Annote = "The main idea seems to be that synchronous message
                  passing won't work, so they advocate asynchronous
                  message passing coupled with software interrupt
                  handlers to receive the messages.  They identify
                  three real-time paradigms: 1) control, in which
                  a master is periodically sending out control
                  messages that must be asynchronously received
                  by slave processes that are continously monitoring
                  some condition.  2) query, in which asynchrounous
                  messages will come in querying the current status.
                  The return message could be half-synchronous.
                  3) WORM, in which an asynchronous message
                  circulates among the processors of the system.
                  The example given here is a token managing
                  some critical resource."}

@inproceedings{cl:pri,
	Author = "Michael J. Carey and Miron Livny",
	Title = "Priority in DBMS Resource Scheduling",
	Booktitle = "Proceedings of the Fifteenth International
		     Conference on Very Large Data Bases",
	Month = "August",
	Year = 1989,
	Address = "Amsterdam, The Netherlands",
	Pages = "397--410",
	Annote = "Considers adding priority to scheduling of disk,
		  CPU, and buffer management.  Conclusion is that
		  if you add priority to disk and CPU, better add
		  it to buffer management as well."}

@techreport{cl:dis,
	Author = "Michael J. Carey and Miron Livny",
	Title = "Distributed Concurrency Control Performance: A Study
		 of Algorithms, Distribution, and Replication",
	Institution = "University of Wisconsin-Madison",
	Month = "March",
	Year = 1988,
	Number = "Computer Sciences Technical Report \#758",
	Annote = "Considers 2PL, Wound-wait, OPT, and Timestamp.
		  The model disregards log writes, and assumes
                  system is running I/O bound.  Conclusions were
                  that 2PL dominates unless message cost is
                  high, in which case OPT does well because of
                  no requirement of communication on every write."} 

@inproceedings{cm:801,
	Author = "Albert Chang and Mark F. Mergen",
	Title = "801 Storage: Architecture and Programming",
	Booktitle = "Proceedings of the 11th ACM Symposium on Operating System
		     Principles",
	Year = 1987,
	Annote = "The goal of the 801 storage is to provide
	large virtual address space with transaction
	capabilities.  Benefits of the storage include
	1) Files directly addressable in linear storage,
	2) Files are shared in storage, and system provides
	   atomic correctness,
        3) Storage functions are unified in single implementation
	   (rather than repeated in several applications.)
        4) System-wide, rather than application specific, buffer
	   pool.

        The hardware page table entries contains lockbits, one for
	each 128 byte part of a page.  There is a transaction id
	register that must match the tid field in page table to
	permit access.
	
	There are also external page tables for pages not in
	memory.
	"}
	

@article{cdkk:diwss,
	Author = "H-T. Chou and David J.~Dewitt and Randy H.~Katz 
	          and Anthony C.~Klug",
	Title = "Design and Implementation of the {Wisconsin} {Storage}
		 {System}",
	Journal = "Software---Practice and Experience",
	Volume = 15,
	Number = 10,
	Month = "October",
	Year = "1985",
	Pages = "943--962",
	Annote = "The Wisconsin Storage System is an expermental
		  vehicle for building a variety of database
		  systems (not just relational.)  Key features
		  seem to be bypassing UNIX by using a raw disk,
		  storing blocks of files continuously in extents,
		  B+-tree indices, and support for long (more than
		  one disk block) data fields.  The long field
		  support consists of breaking the field up
		  into 4K byte sections (``slices'') and including
		  a directory at the head of the field."}

@inproceedings{cabk:dat,
	Author = "George Copeland and William Alexander and
	          Ellen Boughter and Tom Keller",
	Title = "Data Placement in {B}ubba",
	Booktitle = "Proceedings of the SIGMOD International 
	             Symposium on Management of Data",
	Pages = "99--108",
	Year = 1988,
	Month = "May",
	Address = "Chicago, Illinois",
	Annote = "Discusses how to place data to maximize
	         performance in systems with lots of
		 data and many processing elements."}

@inproceedings{cfw:uni,
	Author = "George Copeland and Michael Franklin and Gerhard Weikum",
	Title = "Uniform Object Management",
	Booktitle = "Proceedings of the EDBT Conference",
	Year = 1990,
	Note = "To appear.",
	Annote = "The authors argue that both converting between
		  disk-based formats to in-memory format, and
		  using disk-based formats for in-memory objects,
		  yield poor performance.  Their solution is a
		  single level store.  Some interesting ideas
                  include 1) automatic locking of data pages
		  upon reference 2) two page sizes, one large
                  for disk and a small for in-memory
		  3) objects all created in a transient heap,
                  copied into a persistent heap at the end of
                  transactions 4) large vs. small object
                  distinction, with compiler-generated
                  indexing for large objects."} 

@inproceedings{ckks:the,
	Author = "George Copeland and Tom Keller and Ravi Krishnamurthy
		  and Marc Smith",
	Title = "The Case For Safe {RAM}",
	Booktitle = "Proceedings of the Fifteenth International Conference
		     on Very Large Databases",
	Month = "August",
	Year = "1989",
	Address = "Amsterdam, The Netherlands",
	Pages = "327--335",
	Annote = "Considers the question of when adding battery
		  backup to a portion of RAM is cost-effective.
		  Essentially just uses this RAM to cache commits
		  and checkpoint writes, and then the cache is written
		  out to disk in the background.  Doesn't consider any
		  new algorithms for implementing a system, but uses
		  costs for memory, disk, and batteries to provide
		  some estimates.  The idea is that the batteries have
		  enough power to run the system until the RAM is
		  written to disk.  You have to provide enough RAM
		  so that the odds are the safe RAM will only overflow
		  infrequently (using Poisson arrivals and an M/M/1
		  queuing model.)"}


@inproceedings{ck:ade,
	Author = "George Copeland and Setrag Khoshafian",
        Title = "A Decomposition Storage Model",
	Booktitle = "Proceedings of the ACM SIGMOD International
                     Conference on Management of Data",
	Month = "May",
	Year = 1985,
	Address = "Austin, Texas",
	Pages = "268 -- 279"}


@inproceedings{c:est,
	Author = "Stavros Christodoulakis",
	Title = "Estimating Block Transfers and Join Sizes",
	Booktitle = "Proceedings of the ACM SIGMOD International Conference
	             on Management of Data",
	Year = 1983,
	Address = "San Jose, California",
	Month = "May",
	Pages = "40--54",
	Annote = "He considers how to provide estimates of the number
		  of sequential and random block accesses required
		  to retrieve a number of records when the storage is
		  not uniform.  The basic approach seems to be to
		  maintain summary statistical information, such as
		  the number of records in each block.  Next if
		  you desire to know the number of block accesses
		  to require a given set of $k$ records, return as
		  an estimate the number of blocks necessary to get
		  a random set of $k$ records, based on the statistics
		  maintained about the relation.  He does a similar
		  computation for joins and semi-joins."}


@techreport{christodoulakis:ont,
	Author = "Stavros Christodoulakis",
	Title = "On the Estimation and Use of Selectivities in
		 Database Performance Evaluation",
	Institution = "University of Waterloo Department of Computer Science",
	Number = "CS-89-24",
	Month = "June",
	Year = 1989,
	Annote = "A good survey and some interesting comments on future
	 	  research.  Among the interesting facts: DB typically
		  maintain number of tuples in rel, number of distinct
                  values in a domain, size of constant from domain, etc.
		  He discusses parametric methods (easy to understand,
		  build, and maintain; also easy to make inferences
		  about, but may not fit data) and nonparametric methods,
		  which in his case are algebraic (dist. modeled as by
                  a polynomial) and histogram.  The point is that he is
                  concerned with what statistics the DBMS stores.  He
		  thinks a critical area for future research is
                  sensitivity analysis and error propagation.  One technique
                  for error analysis is majorization and Schur convex
		  functions; this can sometimes prove that an estimate is
                  a lower or upper bound on the actual cost."}

@article{c:imp,
	Author = "Stavros Christodoulakis",
	Title = "Implications of certain assumptions in database
                 performance evaluation",
	Journal = "ACM Transactions on Database Systems",
	Volume = 9,
	Number = 2,
	Month = "June",
	Year = 1984,
	Annote = "The simplifying assumptions he considers are:
		  1. Uniformity of attribute values.
		  2. Independence of attribute values.
		  3. Uniformity of queries.
		  4. Constant number of tuples per page.
		  5. Random placement of tuples within a page."}

	

@article{cggrt:the,
	Author = "W. Crowther and J. Goodhue and E. Starr and R. Thomas",
	Title = "The {B}utterfly ({TM}) Parallel Processor",
	Journal = "IEEE Computer Architecture Technical Committee
		   Newletter",
	Month = "September-December",
	Year = 1985,
	Pages = "18--45"}

@article{ds:dea,
	Author = "William J. Dally and Charles L. Seitz",
	Title = "Deadlock-Free Message Routing in Multiprocessor
		 Interconnection Networks",
	Journal = "IEEE Transactions on Computers",
	Volume = "C-36",
	Number = 5,
	Month = "May",
	Year = 1987,
	Pages = "547--553",
	Annote = "Shows how to give deadlock-free routing algorithms
		  through creating an acyclic virtual network by
		  splitting real channels into virtual channels."} 
	

@inproceedings{dst:dis,
	Author = "Dean S. Daniels and Alfred Z. Spector and
	          Dean S. Thompson",
	Title = "Distributed Logging for Transaction Processing",
	Booktitle = "Proceedings of the ACM SIGMOD International
	             Conference on Management of Data",
	Year = 1987,
	Month = "May",
	Address = "San Fransisco, California",
	Pages = "82--96",
	Annote = "Describes the design of a log-server node to
	          do the logging in a distributed system.  They
	          claim high performance --- I think they mean
		  that it can do lots of logging.  It's not
		  clear that an application can use such a
		  service effectively."}

@inproceedings{d:est,
	Author = "Robert Demolombe",
	Title = "Estimation of the number of tuples satisfying a query
	         expressed in predicate calculus language",
	Booktitle = "Proceedings of the Sixth International Conference
		     on Very Large Databases",
	Address = "Montreal, Canada",
	Year = 1980,
	Pages = "55-63",
	Annote = "Discusses means of producing estimates of query sizes
                  by computations on statistical information about base
                  relations.  Nothing about accuracy or complexity
                  here."}

@article{d:wor,
	Author = "Peter J. Denning",
	Title = "Working sets past and present",
	Journal = "IEEE Transactions on Software Engineering",
	Volume = "SE-6",
	Number = 1,
	Pages = "64--84",
	Year = 1980,
	Month = "January"}

@article{d:O2,
	Author = "O. {Deux et al.}",
	Title = "The $O_2$ System",
	Journal = "Communications of the ACM",
	Volume = 34,
	Number = 10,
	Month = "October",
	Year = 1991}

@unpublished{dnsv:par,
	Author = "D. DeWitt and J. Naughton and J. Shafer and S. Venkataraman",
	Title = "{ParSet} Design Document",
	Month = "November",
	Year = 1993,
	Note = "Unpublished manuscript"}


@article{dg:parold,
	Author = "D. DeWitt and J. Gray",
	Title = "Parallel Database Systems: The Future of Database
		Processing or a Passing Fad",
	Journal = "ACM SIGMOD Record",	
	Volume = 19,
	Number = 4,
	Month = "December",
	Year = 1990}

@inproceedings{dlpy:par,
	Author = "D. DeWitt and J. Luo and J. Patel and J. Yu",
	Title = "Paradise --- A Parallel Geographic Information System",
	Booktitle = "Proceedings of the ACM Workshop on Advances in 
                     Geographic Information Systems",
	Month = "November",
	Year = 1993}

@article{dg:par,
	Author = "D. DeWitt and J. Gray",
	Title = "Parallel Database Systems: The Future of High Performance Database
		Processing",
	Journal = "Communications of the ACM",	
	Year = 1992,
	Month = "June"}

@article{d:dir,
	Author = "D. J. DeWitt",
	Title = "{DIRECT} --- A Multiprocess Organaization for
		 Supporting Relational Database Management Systems",
	Journal = "IEEE Transactions on Computers",
	Month = "June",
	Year = 1979,
	Pages = "395--406"}

@inproceedings{dfmv:ast,
	Author = "David J. DeWitt and Philippe Futtersack and David Maier 
                  and Fernando Velez",
	Title = "A Study of Three Alternative Workstation-Server
                 Architectures for Object-Oreinted Database Systems",
	Booktitle = "Proceedings of the VLDB Conference",
	Month = "August",
	Year = 1990,
	Address = "Brisbane, Australia",
	Annote = "Compares (1) Object-server, (2) Page-server, (3) NFS
                  server on the ACOB benchmark.  Main conclusions:

		(1) object-server relatively insensitive to clustering,
                    main thing is cache size up to a point, then number
                    of RPC's.

                (2) page-server very sensitive to clustering and buffer
                    pool size.

                (3) File server's performance great when reading pages,
                    but terrible when writing due to performance of
                    NFS writes (synchronous!)."}

@inproceedings{dns:aco,
	Author = "David J. DeWitt and Jeffrey F. Naughton and Donovan A. Schneider",
	Title = "A comparison of non-equijoin algorithms",
	Booktitle = "Proceedings of the Eighteenth International Conference
		on Very Large Databases",
	Year = 1991,
	Month = "August",
	Address = "Barcelona, Spain"}

@inproceedings{dnss:pra,
	Author = "David J. DeWitt and Jeffrey F. Naughton and Donovan A. Schneider and S. Seshadri",
	Title = "Practical Skew Handling in Parallel Joins",
	Booktitle = "Proceedings of the Nineteenth International Conference
		on Very Large Databases",
	Year = 1992,
	Month = "August",
	Address = "Vancouver, British Columbia"}

@inproceedings{dns:parallel,
	Author = "David J. DeWitt and Jeffrey F. Naughton and Donovan A. Schneider",
	Title = "Parallel External Sorting using Probabilistic Splitting",
	Booktitle = "PDIS",
	Year = 1991,
	Month = "December",
	Address = "Miami Beach, Florida"}

@inproceedings{dg:mul,
	Author = "David M. DeWitt and Robert Gerber",
	Title = "Multiprocessor Hash-Based Join Algorithms",
	Booktitle = "Proceedings of the Twelfth International Conference
		     on Very Large Databases",
	Year = 1985,
	Pages = "151--164",
	Address = "Stockholm, Sweden",
	Annote = "Provides simulations of the hybrid-hash and GRACE
		  multiprocessor join algorithms.  Not much detail is
		  presented as to how the simulations were run.
		  The upshot was that hybrid is best, and achieves
		  near linear speedup."}

 

@inproceedings{dgghkm:gam,
	Author = "David J. Dewitt and Robert H. Gerber and
	          Goetz Graefe and Michael L. Heytens and
		  Krishna B. Kumar and M. Muralikrishna",
	Title = "{GAMMA} --- A High Performance Dataflow Database
		 Machine",
	Booktitle = "Proceedings of the Twelfth International
                     Conference on Very Large Databases",
	Year = 1986,
	Pages = "228--237",
	Address = "Kyoto, Japan",
	Month = "August",
	Annote = "GAMMA is 20 VAX 11/750's, each with their own
		  disk, connected by an 80 Mbit token ring.  Relations
		  are horizontally partitioned, and joins use
		  hash algorithms.  The author's argue that
		  processors should be put between the disks
		  and the interconnection network, to provide
		  high I/O bandwidth.  They claim good speedup, 
		  although their graphs seem to level off at
		  8 processors."}

@article{dgsbhr:gamma,
	Author = "D. DeWitt and S. Ghandeharizadeh and D. Schneider
                  and A. Bricker and H.-I Hsiao and R. Rasmussen",
	Title = "The {Gamma} database  machine project",
	Journal = "IEEE Transactions on Knowledge and Data Engineering",
	Volume = 2,
	Number = 1,
	Month = "March",
	Year = 1990}

@inproceedings{dgs:ape,
	Author = "David J. DeWitt and Shahram Ghandeharizadeh and
	          Donovan Schneider",
	Title = "A Performance Analysis of the {GAMMA} Database
	         Machine",
	Booktitle = "Proceedings of the SIGMOD International
		     Conference on Management of Data",
	Year = 1988,
	Address = "Chicago, Illinois",
	Month = "May",
	Pages = "350--360",
	Annote = "Really a comparison between Gamma and the Teradata
	          machine.  Benchmarks are on 240 Mbyte database,so
		  MMM could do much better.  Gamma did better than
		  Teradata on almost everything, mainly because
		  1) Teradata uses hash based storage structures,
		     which are rotten for range queries;
		  2) Teradata logs each tuple insertion into 
		     intermediate results.
		  Gamma is good except when its main-memory hash
		  join tables overflow."}


@inproceedings{dkossw:imp,
	Author = "David J. DeWitt and Randy H. Katz and Frank Olken
	          and Lenard D. Shapiro and Michael R. Stonebraker
	          and David Wood",
	Title = "Implementation Techniques for Main Memory Database
		 Systems",
	Booktitle = "Proceedings of the ACM SIGMOD International
		     Conference on Management of Data",
        Year = 1984,
	Month = "June",
	Location = "Boston, Massachusetts",
	Pages = "1--8",
	Annote = "Discusses changes to relational database systems
	          to take advantage of large memories.  Conclusions
		  are that B+ trees are preferred for
                  keyed access to tuples unless $>80\%$ of
		  the database fits in memory (the only alternative
		  they considered was AVL trees).  Also says that
		  hash based strategies for join, project, and
		  aggregate operations dominate if the memory
		  is bigger than the square root of the relations.
		  (Considers simple hash, hybrid, sort-merge, and
		  GRACE.)
		  
		  Also discusses a recovery mechanism based on
		  a log in stable storage, checkpointing done
		  in parallel with transaction activity,
		  and group commits.  They state that `` a typical
		  log data is 400 bytes --- 40 each for begin/end
		  and 360 for old/new values.''"}

@inproceedings{dse:bri,
	Author = "Peter C. Dibble and Michael L. Scott and Carla Schlatter Ellis",
	Title = "Bridge: A High-Performance File System for Parallel Processors",
	Booktitle = "Proceedings of the Eighth International Conference on 
                     Distributed Computer Systems",
	Month = "June",
	Year = "1988",
	Pages = "154 -- 161",
	Annote = "Basic idea: stripe file blocks across processors.  Users are
                  responsible for making this fast if access isn't sequential.
	          (Data mapping to make their accesses sequential, I suppose.)
                  Another couple of ideas: tries to offload FS work to processors
                  with disks; allows users to build sophisticated tools (e.g.
                  parallel sorts, copies, greps.)  Simulated on a BBN Butterfly
                  (no real disks.)"}
                  

@article{d:sor,
	Author = "W. Dobosiewicz",
	Title = "Sorting by distributive partitioning",
	Journal = "Information Processing Letters",
	Volume = 7,
	Number = 1,
	Pages = "1--6",
	Year = 1978,
	Annote = "Algorithm: to sort n records,
		1. Find max, min, median in O(n) time.
		2. Divide data into n buckets, n/2 to left of median,
		   n/2 to right.
		3. Scan, placing each record into appropriate bucket.
		4. Recurse on any bucket with more than 1 element.
		He shows that this has O(n) expected time if the
		data is uniformly distributed."}

@inproceedings{dd:ape,
	Author = "J. Duhl and C. Damon",
	Title = "A Performance Comparison of Object and Relational
                 Databases Using the Sun Benchmark",
	Booktitle = "Proceedings of the ACM OOPSLA Conference",
	Address = "San Diego, California",
	Month = "September",
	Year = 1988}

@article{eh:pri,
	Author = "Wolfgang Effelsberg and Theo Haerder",
	Title = "Principles of Database Buffer Management",
	Journal = "ACM Transactions on Database Systems",
	Year = 1984,
	Volume = 9,
	Number = 4,
	Month = "December",
	Pages = "560--595",
	Annote = "A survey paper.  Notes that database reference
	          strings are different than program reference
		  strings, and that the OS can get in the way
		  by causing double page faults, one for
		  the buffer miss, the other to bring in the
		  page of OS space in which the data page belongs."}

@inproceedings{e:mai,
	Author = "Margaret H. Eich",
	Title = "Main Memory Database Recovery",
	Booktitle = "Proceedings of the ACM-IEEE Fall
	             Joint Computer Conference",
        Year = 1986}

@inproceedings{e:acl,
	Author = "Margaret H. Eich",
	Title = "A Classification and Comparison of Main Memory
	         Database Recovery Techniques",
	Booktitle = "Proceedings of the Third International Conference
	             on Data Engineering",
	Year = 1987,
	Pages = "332--339",
	Annote = "Argues that the primary factors determining
		  performance are 1) whether there is enough
		  stable storage for the entire database, or just
		  the log, or none; 2) whether or not there is
		  special logging hardware; 3) whether or not
		  there is checkpointing overhead 4) commit
		  policy (group, immediate, or pre - irrelevant
		  if there is stable storage.)
		  
		  The conclusion is that it is most important
		  to have enough stable storage for the log.
		  Adding more doesn't help much.  Next is
		  specialized logging hardware.  Finally, group
		  commit is too expensive for throughput."}

@inproceedings{esw:dis,
	Author = "Robert Epstein and Michael Stonebraker and Eugene
		  Wong",
	Title = "Distributed Query Processing in a Relational
		 Database System",
	Booktitle  = "Proceedings of the ACM-SIGMOD International
		      Conference on Management of Data",
	Year = 1978,
	Annote = "Presents an algorithm for retrieving and updating
		  data in a distributed system.  Essentially,
		  breaks query into pieces, and distributes
		  the pieces (essentially ``one variable queries'').
		  Involves broadcasting relations to all sites.
		  One relation is horizontally partitioned, while
		  all other relations are broadcast."}

@book{gr:tra,
	Author = "Jim Gray and Andreas Reuter",
	Title = "Transaction Processing: concepts and techniques",
	Publisher = "Morgan Kaufmann",
	Address = "San Mateo, CA",
	Year = 1993}

@techreport{k:mul,
	Author = "David Kotz",
	Title = "Multiprocessor File System Interfaces",
	Institution = "Dartmouth College",
	Address = "Hanover, New Hampshire",
	Month = "March",
	Year = "1992",
	Number = "PCS-TR92-179 (revised)",
	Annote = "New interface allows for parallel open, synchronization
                  for global file access patterns (through shared
                  file pointers?), mapped file pointers (map [filep, param]
                  pairs to positions in a file), support for logical 
                  records (to get rid of false sharing, give unit for
                  consistency, etc.), and multifiles (logical one file,
                  but each process in a group computation sees its own
                  separate file.)"}

@article{ke:pre,
	Author = "David F. Kotz and Carla Schlatter Ellis",
	Title = "Prefetching in File Systems for {MIMD} Multiprocessors",
	Journal = "IEEE Transactions on Parallel and Distributed Systems",
	Volume = 1,
	Number = 2,
	Month = "April",
	Year = 1990,
	Pages = "218--230",
	Annote = "Addresses the problem of how prefetching helps in
                  MIMD systems.  Through simulation, points out that
		  prefetching might not be a huge help because (1)
		  if many processors are going after the same date
		  in sequential order, 'hits' might actually be hits
		  of empty buffers waiting on an I/O, and (2) the 
                  overhead of prefetching might be harder to hide in
		  systems where individual nodes are not multiprogrammed,
		  and (3) speeding up one process doesn't help anyway if
                  it must wait for others for synchronization.  Results
                  obtained on a multiprocessor with simulated disks."}

@inproceedings{ke:pra,
	Author = "David F. Kotz and Carla Schaltter Ellis",
	Title = "Practical Prefetching Techniques for Parallel File Systems",
	Booktitle = "Proceedings of the First International Conference on Parallel
                     and Distributed Information Systems",
	Month = "December",
	Year = "1992",
	Address = "Miami Beach, Florida",
	Pages = "182--189",
	Annote = "Proposes a classification of access patterns (main 
                  discriminant: local sequential vs. global sequential)
                  and investigates the performance of prefetching techniques.
                  The prefetching techniques are a combination of look ahead,
                  and smarter lookaheads that discover 'strides' in file
                  access.  They found that their prefetch techniques approached
                  the optimal 'full knowledge' prefetching techniques in 
                  their experiments on a BBN Butterfly with simulated I/O."}

@article{f:cap,
	Author = "R. S. Fabry",
	Title = "Capability-Based Addressing",
	Journal = "Communications of the ACM",
	Month = "July",
	Year = 1974,
	Volume = 17,
	Number = 7,
	Pages = "403--412",
	Annote = "The author argumes that if we wish to construct a
		time-varying multisegment data structure containing
		internal cross-references and having an existence
		independent of any particular program or process, the
		only alternative is to use absolute addresses.  Capabilities
		can serve as these addresses; the paper gives some ways
		of implementing these capabilities."}

@inproceedings{fl:ade,
	Author = "A. A. Faustini and E. B. Lewis",
	Title = "A Declarative Language for the Specification of
	         Real Time Systems",
	Booktitle = "Proceedings of the IEEE Real-Time Systems
		     Symposium",
	Year = 1985,
	Month = "December",
	Address = "San Diego, California",
	Pages = "43--51",
	Annote = "The authors describe an extension of Lucid to
		  deal with time.  Lucid is stream-based; their
		  extension allows creation and destruction intervals
		  for objects."}

@inproceedings{f:dat,
	Author = "J. Fedorowicz",
	Title = "Database performance evaluation using multiple regression 
		 techniques",
	Booktitle = "Proceedings of the International ACM SIGMOD Conference",
	Year = 1984,
	Month = "June",
	Address = "Boston, Massachusetts",
	Pages = "70--76",
	Annote = "Apparently, proposes Zipf distribution for words in
                  text."}

@techreport{fz:iss,
	Author = "E. Felten and J. Zahorjan",
	Title = "Issues in the Implementation of a Remote Memory Paging
                 System",
	Institution = "University of Washington",
	Number = "91-03-09",
	Month = "March",
	Year = 1991}

@unpublished{fm:new,
	Author = "Edward W. Felten and Dylan McNamee",
	Title = "NewThreads 2.0 User's Guide",
	Institution = "University of Washington --- Department of
                       Computer Science and Engineering",
	Month = "August",
	Year = 1992}

@inproceedings{flw:ove,
	Author = "Daniel H. Fishman and Ming-Yee Lai and 
	          W. Kevin Wilkinson",
	Title = "Overview of the {Jasmin} Database Machine",
	Booktitle = "Proceedings of the ACM SIGMOD International
	             Conference on Management of Data",
	Year = "1984",
	Pages = "234--239",
	Address = "Boston, Massachusetts",
	Month = "June",
	Annote = "Jasmin's promary goal is to demonstrate
	          the feasibility of a practical multiprocessor
		  database machine suitable for large database,
		  high transaction-rate applications.  Their
		  machine has nodes (collections of processors
		  sharing a memory-addressed bus) connected on
		  a LAN.  This paper mostly just gives the
		  names of the software modules in their
		  system."}

@inproceedings{fp:mir,
	Author = "Brett D. Fleisch and Gerald J. Popek",
	Title = "Mirage: A Coherent Distributed Shared Memory Design",
	Booktitle = "Proceedings of the Twelfth ACM Symposium on Operating
                     System Principles",
	Year = 1989,
	Month = "December",
	Address = "Litchfield Park, Arizona",
	Pages = "211--223",
	Annote = "Describes an implementation of SVM.  The implementation is
                  kernel level, in the Locus OS.  They use a simple scheme ---
                  fixed owner, single writer, multiple readers.  One 
                  new approach: a delta time limit during which a user
                  (reader or writer) of a page cannot be interrupted for
                  invalidation/downgrading of permissions."}

@inproceedings{fbs:the,
	Author = "Alessandro Forin and Joseph Barrera and Richard Sanzi",
	Title = "The Shared Memory Server",
	Booktitle = "Proceedings of the Winter {USENIX} Technical Meeting",
	Month = "January",
	Year = 1989,
	Address = "San Diego, California",
	Pages = "229--244",
	Annote = "Discusses implementing SVM using Mach external pagers.
		  The basic idea is to treat remote pagers as other kernels.
                  At all times every kernel knows the owner of every page.
		  Forwarding is used for efficiency.  Benchmarks the
                  system on up to 3 processors!!"}

@inproceedings{fztcd:cra,
	Author = "Michael J. Franklin and Michael J. Zwilling and 
                  C. K. Tan and Michael J. Carey and David J. DeWitt",
	Title = "Crash Recovery in Client-Server {EXODUS}",
	Booktitle = "Proceedings of the ACM-SIGMOD Conference on the
                     Management of Data",
	Month = "June",
	Year = 1992,
	Pages = "165--174"}

@inproceedings{fc:cli,
	Author = "M. Franklin and M. Carey",
	Title = "Client-Server Caching Revisited",
	Booktitle = "Proceedings of the International Workshop on 
                     Distributed Object Management", 
	Address = "Edmonton, Canada",
        Month = "August", 
	Year = 1992,
        Note = "(published as {\em Distributed Object Management},
Ozsu, Dayal, Vaduriez, eds., Morgan Kaufmann, San Mateo, CA, 1993)."
}

@phdthesis{f:cac,
	Author = "M. Franklin",
	Title = "Caching and Memory Management in Client-Server 
                 Database Systems",
	School = "University of Wisconsin Computer Sciences Dept.",
	Month = "August",
	Year = 1993,
	Note = "TR \#1168"}




@article{fm:sam,
	Author = "W. D. Frazer and A. C. McKellar",
	Title = "Samplesort: A Sampling approach to minimal storage tree sorting",
	Journal = "Journal of the ACM",
	Volume = 17,
	Number = 3,
	Pages = "496 -- 507",
	Year = 1970}

@inproceedings{fkt:ano,
	Author = "Shinya Fushimi and Masaru Kitsuregawa and
	          Hidehiko Tanaka",
	Title = "An Overview of the System Software of A
	         Parallel Relational Database Machine {GRACE}",
	Booktitle = "Proceedings of the Twelfth International 
	             Conference on Very Large Databases",
	Pages = "209--219",
	Year = 1986,
	Annote = "These guys claim that with high performance
	          disks and a multi-dimensional clustering
		  algorithm, the I/O bottleneck has been
		  replaced by the heavy control
		  and data transfer overhead.  Their
		  data stream control protocol supposedly
		  eliminates this overhead."}

@article{glv:ama,
	Author = "Hector Garcia-Molina and Richard J. Lipton
	          and Jacobo Valdez",
	Title = "A Massive Memory Machine",
	Journal = "IEEE Transactions on Computing",
	Volume = "C-33",
	Pages = "391--399",
	Month = "May",
	Year = 1984}

@inproceedings{gk:tra,
	Author = "Jorge F. Garza and Won Kim",
	Title = "Transaction Management in an Object Oriented
	         Database System",
	Booktitle = "Proceedings of the SIGMOD International
		     Symposium on Management of Data",
	Year = 1988,
	Month = "May",
	Address = "Chicago, Illinois",
	Pages = "37--45",
	Annote = "The main issue seems to be how to lock classes
	          and instances, and how to decide various
		  compatibilites such as read lock on class and
		  write lock on instance.  Their system also
		  (ORION) supports hypothetical transactions, which
		  always abort.  These are supported by making
		  copies of everything the transaction touches,
		  which doesn't seem too efficient..."}
	

@article{g:exp,
	Author = "Gaston H. Gonnet",
	Title = "Expected length of the longest probe sequence in hash
		code searching",
	Journal = "JACM",
	Volume = 28,
	Number = 2,
	Pages = "289 -- 304",
	Month = "April",
	Year = 1981}

@article{gk:var,
	Author = "Dieter Gawlick and David Kinkade",
        Title = "Varieties of Concurrency Control in {IMS/VS} {Fast}
                 {Path}",
	Journal = "Database Engineering Bulletin",
	Volume = 8,
	Number = 2,
	pages = "3--10",
        Month = "June",
	Year = 1985}

@inproceedings{g:var,
	Author = "Edward F. Gehringer",
	Title = "Variable-Length Capabilities as a Solution
	         to the Small-Object Problem",
	Booktitle = "Some ACM Proceedings",
	Year = 1979,
	Pages = "131--142",
	Annote = "The solution seems to involve grouping sets
	          of similar objects to store redundant
		  capability information once per set.  Has
		  an early capability system bibliography."}

@inproceedings{gg:the,
	Author = "E. Gelenbe and D. Gardy",
	Title = "The size of projections of relations satisfying a 
                 functional dependency",
	Booktitle = "Proceedings of the 8th International VLDB
		     Conference",
	Year = "1982",
	Address = "Mexico City",
	Pages = "325--323"}

@inproceedings{g:usi,
	Author = "James R. Goodman",
	Title = "Using cache memory to reduce processor-memory
	         traffic",
	Booktitle = "Proceedings of the Tenth Annual International
	             Symposium on Computer Architecture",
	Month = "May",
	Year = "1983",
	Address = "Stockholm, Sweden",
	Pages = "124--131",
	Annote = "Introduced ``Write-once,'' the first published
                  coherence strategy for shared-bus multicomputers.
		  See paper by Archibald and Baer above."}

@techreport{g:ani,
	Author = "James R. Goodman",
	Title = "An Investigation of Multiprocessor Structures and
		 Algorithms for Database Management",
	Institution = "University of California at Berkeley",
	Month = "May",
	Year = 1981,
	Number = "UCB/ERL M81/33",
	Annote = "Not read.  Apparently studied hash-basd algorithms
		  in connection with an X-tree interconnect."}

@article{ggkmrs:the,
	Author = "Allan Gottlieb and Ralph Grishman and Clyde P. Kruskal
		  and Kevin P. McAuliffe and Larry Rudolf and Marc Snir",
	Title = "The {NYU} {U}ltracomputer --- Desiging an {MIMD} Shared
	         Memory Parallel Computer",
	Journal = "IEEE Transactions on Computers",	
	Volume =  "C-32",
	Number = "2",
	Pages = "175--189",
	Month = "February",
	Year = 1983}

@misc{g:personal,
	Author = "Goetz Graefe",
	Note = "Personal Commuication."}

@inproceedings{gd:the,
	Author = "Goetz Graefe and David J. DeWitt",
	Title = "The {EXODUS} Optimizer Generator",
	Booktitle = "Proceedings of the ACM SIGMOD Conference",
	Address = "San Francisco, California",
	Month = "May",
	Year = "1987",
	Pages = "160--171"}

@techreport{g:sor,
	Author = "Goetz Graefe",
	Title = "Parallel External Sorting in Volcano",
	Institution = "University of Colorado - Boulder",
	Month = "March",
	Year = 1990,
	Number = "CU-CS-459-90"}
	
	

@incollection{g:not,
	Author = "J. N. Gray",
	Title = "Notes on Data Base Operating Systems",
	Booktitle = "Operating Systems: An Advanced Course",
	Year = 1979,
	Editor = "R. Bayer and R. M. Graham and G. Seegmuller",
	Publisher = "Springer-Verlag",
	Annote = "A collection of notes about implementing database
	          systems.  Lots of early concurrency stuff.  In
		  particular, he defines level-3 consistency to
		  be 1) freedom from lost updates (write-write
		  dependencies; 2) freedom from dirty reads
		  (write-read dependencies); 3) un-repeatable
		  reads (read-write dependencies.)"}


@article{gea:the,
	Author = "Jim N. {Gray et al.}",
	Title = "The recovery manager of the {System R} Database Manager" ,
	Journal = "ACM Computing Surveys",
	Volume = 13,
	Number = 2, 
	Pages = "223 -- 242",
	Month = "June",
	Year = 1981}



@techreport{gc:lea,
	Author = "Cary G. Gray and David R. Cheriton",
	Title = "Leases: An Efficient Fault-Tolerant Mechanism for
                 Distributed File Cache Consistency",
	Institution = "Computer Science Department, Stanford University",
	Number = "STAN-CS-90-1298",
	Month = "January",
	Year = 1990,
	Annote = "Idea: a client caching a datum gets a lease.  During
                  the period of the lease, the client has control over
                  the datum; at the end of the lease, it loses it.
                  Conclusion, based on admittedly simplified analytical
                  model, is that short leases get most of the gains
                  at a fraction of the costs."} 
		  		  

@inproceedings{gaclp:acc,
	Author = "{P. Griffiths} Selinger and M. M. Astrahan and
		  D. D. Chamberlin and R. A. Lorie and T. G. Price",
	Title = "Access path selection in a relational database
		 management system",
	Booktitle = "Proceedings of the ACM SIGMOD International Symposium
		     on Management of Data",
	Year = 1979,
	Pages = "23--34",
	Annote = "A discussion of the query-planning portion of System R.
		  Includes a discussion of cost estimation.  They assumed
		  a uniform distribution between min and max for each
	  	  column, and used simple interpolation formulas to 
		  estimate the size of an operation.  One observation
		  was that although this often provided estimates that
		  were way off, they still managed to choose the best
		  query plan."}

@techreport{gl:elf,
	Author = "Andrew S. Grimshaw and Edmond C. Loyot",
	Title = "{ELFS}: Object-Oriented Extensible File Systems",
	Institution = "Computer Science Department, University of Virginia",
	Number = "TR-91-14",
	Month = "July",
	Year = 1991,
	Annote = "The basic idea is to use OO methodology to allow
                  users to specify different file types (e.g., matrix,
                  parallel file object (striped),...  The hope is that
                  by using these different file types much better performance
                  will result.  E.g., for the matrix file type, by 
                  storing data by squares instead of by rows or columns,
                  both row and column operation scan be supported much
                  more efficiently."}

@inproceedings{gp:hig,
	Author = "Andrew S. Grimshaw and Jeff Prem",
	Title = "High Performance Parallel File Objects",
	Booktitle = "Proceedings of the Sixth Distributed Memory
                     Computing Conference",
	Month = "April",
	Year = 1991,
	Address = "Portland, Oregon",
	Pages = "720 -- 723",
	Annote = "Very brief description of ELFS Parallel File Objects
                  (pfo's).  The performance on a basic benchmark
                  (numerous readers reading an array row by row, with
                  stride equal to the number of readers) show that it
                  can beat CFS by a large factor.  The main reason appears
                  to be that there is no way to tell CFS what you are
                  going to do, so the whole file is scanned by every
                  reader in CFS, while each reader only gets its chunk
                  under ELFS.  The OO formalism is a nice way of 
                  packaging all of this."}

@article{h:acr,
	Author = "Robert B. Hagmann",
	Title = "A Crash Recovery Scheme for a Memory-Resident
                 Database System",
	Journal = "IEEE Transactions on Computers",
	Volume = "C-35",
	Number = 9,
	Pages = "839--843",
	Month = "September",
	Year = 1986,
	Annote = "Advocates fuzzy checkpointing, noticing that
	          applying log entries is idempotent.  Because
		  he does not consider keeping two copies
		  of DB in memory, his scheme cannot be
		  used with pre-commit policies."}


@article{hr:pri,
    Author = "Theo Haerder and Andreas Reuter",
    Title = "Principles of Transaction-Oriented Database Recovery",
    Journal = "Computing Surveys",
    Volume = 15,
    Number = 4,
    Month = "December",
    Year = 1983,
    Pages = "287--317"}

@inproceedings{hckw:apr,
	Author = "Eric N. Hanson and Moez Chaabouni and Chang-Ho Kim
                  and Yu-Wang Wang",
	Title = "A Predicate Matching Algorithm for Database Rule Systems",
	Booktitle = "Proceedings of the ACM-SIGMOD International Conference
                     on Management of Data",
	Year = 1990,
	Month = "May",
	Address = "Atlantic City, New Jersey",
	Pages = "271--280",
	Annote = "Considers the following problem: given a tuple and
                  a collection of predicates, which of those predicates
                  is satisfied by the tuple?  The solution given is to
                  first hash predicates by relation name.  This gives
                  1) a linked list of predicates that aren't ranges, and
                  2) for each range indexable attribute, an {IBS}-tree
                  that is a data structure containing intervals annotated
                  by the predicates corresponding to the intervals."}
	


@inproceedings{hms:rec,
	Author = "R. Haskin and Y. Malachi and W. Sawdon",
	Title = "Recovery management in {QuickSilver}",
	Booktitle = "Proceedings of the 11th ACM Symposium on Operating
		     System Principles",
	Year = 1987}

@inproceedings{h:rdb,
	Author = "W. Hell",
	Title = "{RDBM} --- {Relational} {Database} {Machine}:
	         Architecture and Hardware Design",
	Booktitle = "Sixth Workshop on Computer Architecture for
	             Nonnumerical Processing",
	Month = "June",
	Year = 1981}

@inproceedings{hslcgr:gro,
	Author = "Pat Helland and Harald Sammer and Jim Lyon 
	          and Richard Carr and Phill Garrett and 
		  Andreas Reuter",
	Title = "Group  commit timers and high volume transaction
	         systems",
        Booktitle = "Proceedings of the Second International Workshop
	             on High Performance Transaction Systems",
	Month = "September",
	Year = 1987,
	Annote = "The authors propose varying time group commit
	          timers to respond to varying system loads."}

@inproceedings{h:rel,
	Author = "M. V. Hermenegildo",
	Title = "Relating Goal Scheduling, Precedence, and
		 Memory Management in {AND}-parallel Execution of
		 Logic Programs",
	Booktitle = "Proceedings of the International Conference on
		     Logic Programming",
	Address = "Melbourne, Australia",
	Year = 1987,
	Pages = "556--576",
	Annote = "Gives a scheme so that 1. memory can be completely
		  reclaimed on failure (backtracking) and 2. goals
		  are stacked in order of newness.  The goal of all
		  this is to achieve efficient memory usage, avoiding
		  the all-heap solution without introducing garbage
		  slots and trapped goals."}

@techreport{hmgrn:ano,
	Author = "M. Hermenegildo and K. Muthukumar and K. Greene
		  and F. Rossi and R. Nasr",
	Title = "An Overview of the {PAL} Project",
	Year = 1989,
	Month = "June",
	Institution = "MCC",
	Number = "ACT-ST-234-89",
	Annote = "The PAL project seeks to develop new 
	 	  concepts and techniques that can be of general
		  applicability in program analysis and performance 
		  improvement of declarative, knowledge-based, and
		  formal systems.  The main path is the combined use
		  of automatic parallelism, user-specified parallelism,
		  and advanced compilation techniques such as abstract
		  interpretation.  Main focus currently is on 
		  independent AND parallelism.  This paper is sort
		  of a readers digest guide to other TRs that contain
		  more technical meat."}

@inproceedings{hr:ont,
	Author = "Manuel Hermenegildo and Francesca Rossi",
	Title = "On the Correctness and Efficiency of Independent
		 And-Parallelism in Logic Programs",
	Booktitle = "Proceedings of the North American Conference
		     on Logic Programming",
	Year = 1989,
	Annote = "They formally prove that for a suitable definition 
                  of independence, SLD-resolution can be extended to
		  select independent goals in parallel, and that this
		  process doesn't do any more work than the serial
		  implementation of SLD-resolution and can reduce
		  running time.  They also give some techniques
		  for inferring independence."}

@article{hw:des,
	Author = "M. V. Hermenegildo and R. A. Warren",
	Title = "Designing a High Performance Parallel Logic
		 Programming System",
	Journal = "Computer Architecture News",
	Month = "March",
	Year = 1987,
	Annote = "Gives a good description
		  of the RAP-WAM, and states that the goal of
		  parallel LP research  is the preservation of
		  sequential performance and low-overhead control
		  mechanisms for parallelism.  Gives a research
		  methodology for parallel logic programs: 1.
		  Analyze opportunities for parallelism and select
		  one that can be reasonably implemented. 2. 
		  Design an execution model that supports this
		  parallelism with minimum overhead while
		  preserving sequential inferencing speed.  3.
		  Perform a detailed simulation under idealized 
		  architectural parameters.  Find critical operations
		  and optimize them.  4.  Select a particular architecture,
		  use the actual parameters in the simulation, and tune.
		  5.  Implement."}

@article{hs:dat,
	Author = "W. Daniel Hillis and Guy Steele",
	Title = "Data Parallel Algorithms",
	Journal = "Communications of the ACM",
	Month = "December",
	Year = 1986,
	Volume = 29,
	Number = 12,
	Pages = "1170--1183",
	Annote = "Data parallel algorithm are those whose parallelism
	          comes from simultaneous operations across large
		  sets of data rather than from multiple threads of
		  control.  They give several examples of these
		  algorithms --- parsing, sorting, finding the end
		  of a linked list, etc."}

@article{hz:ash,
	Author = "Mark F. Hornick and Stanley B. Zdonik",
	Title = "A Shared, Segmented Memory System for an 
                 Object-Oriented Database",
	Journal = "ACM Transactions on Office Information Systems",
	Volume = "",
	Number = "",
	Month = "",
	Pages = "",
	Annote = "No note."}

@inproceedings{hod:err,
Author = "W. C. Hou and G. Ozsoyoglu and E. Dogdu",
Title = "Error constrained count query evaluation in relational databases",
Booktitle = "Proceedings of the SIGMOD International Conference on
  Management of Data", 
Pages  = "278--287", 
Address = "Denver, Colorado", 
Month = "May",
Year = "1991"}

@article{hkmnssw:sca,
author = "J. Howard and M. Kazarand S. Menees and D. Nichols and
          M. Satyanarayanan and R. Sidebotham and M. West", 
title = "Scale and Performance in a Distributed File System", 
journal = "ACM Transactions on Computer Systems",
Volume = 6,
Number = 1,
Month = "February", 
Year = 1988}

@techreport{ht:tra,
	Author = "Meichun Hsu and Va-On Tam",
	Title = "Transaction Synchronization in Distributed
		 Shared Virtual Memory Systems",
	Year = 1989,
	Number = "TR-05-89",
	Institution = "Center for Research in Computing Technology,
		       Harvard University",
	Annote = "This report studies the interaction between
		  transaction synchronization and memory coherence
		  algorithms in SVM systems.  The main observation
		  is that the two mechanisms can perform redundant
		  operations and can miss optimizations if done
		  independently, for example 2-phase locking on
		  top of the memory coherence.  Some preliminary
		  simulations suggest that incorporating the
		  two by sending data objects when they are
		  locked (instead of when the page fault
		  occurs) is better than naive, and optimistic
		  approaches are still better."}

@inproceedings{htb:upd,
	Author = "Meichun Hsu and Van-On Tam and Matthew Bellew",
	Title = "Update propagation in a distributed memory hierarchy",
	Booktitle = "UNKNOWN",
	Date = "UNKNOWN"}

@inproceedings{hl:han,
	Author = "Kien A. Hua and Chiang Lee",
	Title = "Handling Data Skew in Multiprocessor Database Computers
                 using Partition Tuning",
	Booktitle = "Proceedings of the 17th International Conference on
                     Very Large Databases",
	Address = "Barcelona, Spain",
	Month = "August",
	Year = 1991,
	Pages = "525--535",
	Annote = "Considers three algorithms for handling skew:
	1. Tuple Interleaving Parallel Hash Join.

	Scan relation, spreading tuples everywhere, compute how
        large each hash bucket for a given hash function would be,
	then allocate buckets to processors in such a way as to
	balance the load.

	Bucket sizes are determined by assuming that after buckets
	have been spread, the situation at every processor is representative
	of the global situation.  Partitions are then allocated to
	processors in a best-fit largest-first manner.

	2. Adaptive Load Balancing Parallel Hash Join.

	Scan relation, hash using 'final' hash function, redistribute
	buckets before actually joining.

	In more detail, after initial hash and redistribution, each
	processor keeps as many of the 'sub-buckets' as fit nicely 
	on it, then classifies the remaining buckets as 'excess.'
	Then the excess buckets are reallocated and redistributed
	as in the tuple interleaving strategy.

	3. Extended Adaptive Load Balancing Parallel Hash Join.

	Locally hash and write to disk using 'final' hash function.
	Then compute good global allocation and redistribute.

	The performance analysis used compares these three algorithms
	to GRACE (why not Hybrid??).  The model of skew is 'one bucket
	skewed, rest not skewed.'  The paper uses an analytic model.

	The result is that 1. and 3. give the same performance.  2.
	is only marginally better than GRACE, which in turn is better
	than 1. and 3. except when the skew is large.  These should
	have been compared with Hybrid!!  Also, no scaleup or speedup
	results are given - just a single datapoint.  Finally, they
	show that 3. is better than 1. when the network is the
	bottleneck.  

	The paper ignores the 'ridiculous skew' case (one join attribute
	value so skewed as to be sent to multiple processors.) "}

@inproceedings{hc:par,
	Author = "J. S. Huang and Y. C. Chow",
	Title = "Parallel Sorting and Data Partitioning by Sampling",
	Booktitle = "Proceedings of the Seventh International Computer
		     Software and Applications Conference",
	Pages = "627 -- 631",
	Month = "November",
	Year = 1983,
	Address = "Chicago, Illinois",
	Annote = "Investigates a parallel sample sort.  The idea
		is that each processor takes $l$ samples; these
		are sorted and then used as partitioning elements;
		then the extra sampling elements are used as
		pivots for each of the local sorts.

		The paper doesn't describe any implementation; mostly
		it is a bunch of hairy combinatorial formulas proving
		that the performance is asymptotically good."} 

@inproceedings{ilp:mem,
        author = "Liviu Iftode and Kai Li and Karin Petersen",
        title = "Memory Servers for Multicomputers",
        booktitle = "Proceedings of the IEEE Spring COMPCON '93",
        month = "February",
        year = "1993"}


@techreport{icft:ber,
	Author = "Yannis E. Ioannidis and Joanna Chen and Mark A.
	          Friedman and Manolis M. Tsangaris",
	Title = "{BERMUDA}: An Architectural Perspective on
	         Interfacing Prolog to a Database Machine",
	Year = 1987,
	Month = "October",
	Number = "Computer Sciences 723",
	Institution = "University of Wisconsin-Madison",
	Annote = "No note."}

@inproceedings{ik:ran,
	Author = "Yannis E. Ioannidis and Younkyung Kang",
	Title = "Randomized Algorithms for Optimizing Large Join Queries",
	Booktitle = "Proceedings of the ACM-SIGMOD Conference on the
                     Management of Data",
	Month = "May",
	Year = 1990,
	Pages = "312--321"}

@inproceedings{i:ont,
	Author = "Yannis E. Ioannidis",
	Title = "On the Computation of the Transitive Closure of
	         Relational Operators",
	Booktitle = "Proceedings of the Twelfth International Conference
                     on Very Large Databases",
	Address = "Kyoto, Japan",
	Month = "August",
	Year = 1986,
	Pages = "403--411"}

@inproceedings{ir:eff,
	Author = "Yannis E. Ioannidis and Raghu Ramakrishnan",
	Title = "Efficent Transitive Closure Algorithms",
	Booktitle = "Proceedings of the Fourteenth International Conference
                     on Very Large Databases",
	Address = "Los Angeles, California",
	Year = 1988,
	Month = "August",
	Pages = "382--394"}

@inproceedings{irv:per,
	Author = "Balakrishna R. Iyer and Gary R. Ricard and Peter J. Varman",
	Title = "Percentile Finding Algorithm for Multiple Sorted Runs",
	Booktitle = "Proceedings of the Fifteenth International Conference
		     on Very Large Databases",
	Month = "August",
	Year = "1989",
	Address = "Amsterdam, The Netherlands",
	Pages = "135 -- 144",
	Annote = "Considers the following problem: you have $m$ sorted runs,
		each with $N$ elements.  You want to find the $p$ percentile
		of the union of the runs.  Their algorithm runs in
		$(m\log m)\log(N)$ comparisons, $O(m\log(N))$ I/Os."}

@article{jl:ana,
	Author = "Philip J. Janus and Edmund A. Lamagna",
	Title = "An Adaptive Method for Unknown Distributions in
		 Distributive Partitioned Sorting",
	Journal = "IEEE Transactions on Computers",
	Volume = "C-34",
	Number = 4,
	Month =  "April",
	Year = 1985,
	Pages = "367--372",
	Annote = "A fix for Distributive Partitioned Sorting: first
		sample to approximate the cummulative distribution
		function of the data, then partition based upon this
		approximate CDF instead of assuming a uniform 
		distribution.  The sampling strategy is to
		use proportional sample statistics and the standard
		normal distribution for 3 percent proportions (since
		they divided the range into 31 subintervals) with 90\%
		confidence.  Experimentally, the method worked well."}

@inproceedings{jlt:ati,
	Author = "E. Douglas Jensen and C. Douglass Locke and 
		  Hideyuki Tokuda",
	Title = "A Time-Driven Scheduling Model for Real-Time Operating
		 Systems",
	Booktitle = "Proceedings of the IEEE Real-Time Systems
		     Symposium",
	Year = 1985,
	Month = "December",
	Address = "San Diego, California",
	Pages = "112--122",
	Annote = "Difference between scheduling in OS and OR:
		  in OS, jobs arrive at unpredicatable times
                  and have unpredictable running times.  Real-time
		  systems add the twist that the result of a process
		  has a value to the system that depends on time.
		  In hard real-time systems, deadlines must
		  absolutely be met or else catastrophic results.
		  In soft real-time systems, some deadlines may be
		  occasionally missed causing degradation but
		  not total failure.  Most real-time OS extremely
		  simple, with no virtual memory, limited I/O support,
		  and scheduling based on FIFO, fixed priority,
		  or round-robin.  Real-Time behavior is ensured
		  by static tweaking and extensive testing.

		  They attempt to benchmark scheduling algorithms
		  on a random set of real-time processes.  
		  The algorithms they test are several classical
		  algorithms, such as SPT, Deadline, Slack, 
		  FIFO, Random, RandPRTY, and a couple of new
		  algorithms, a greedy algorithm that always
		  picks the process with the greatest 
		  value to remaining computation ratio, and another
		  that removes low value processes until a
		  set for which all deadlines can be met is
		  produced.  They tested each with four different
		  value functions."}

@inproceedings{j:ada,
	Author = "A. Joshi",
	Title = "Adaptive Locking Strategies in a Multi-Node Data Sharing
                 Environment", 
	Booktitle = "Proc. 17th VLDB Conf.", 
	Address = "Barcelona, Spain",
	Month = "Sept.", 
	Year = 1991}

@unpublished{jz:rec,
	Author = "David B. Johnson and Willy Zwaenepoel",
	Title = "Recovery in Distributed Systems Using Optimistic
		 Message Logging and Checkpointing",
	Institution = "Department of Computer Science, Rice University",
	Note = "Extended version of a PODC paper",
	Annote = "Gives a careful analysis of system states and dependencies
		in a distributed system using message logging and checkpointing
		to provide fault tolerance.  Focusses on optimistic methods;
		extends Strom and Yemini by 1) guaranteeing that it finds
 		the most recent recoverable state after failure, and 2)
		requiring less global information.  In particular, instead
		of prepending a dependency vector with as many entries as
		there are communicating processes, prepends only the current
		state number of the sending process."}


@phdthesis{k:par,
	Author = "Michael F. Kilian",
	Title = "Parallel Sets: An Object-Oriented Methodology for 
                 Massively Parallel Programming",
	School = "Harvard Center for Research in Computing Technology",
	Address = "Cambridge, MA",
	Year = 1992}


@inproceedings{k:thed,
	Author = "Krishnamurthy Kannan",
	Title = "The Design of  a Mass Memory for a Database
Computer",
	Booktitle = "The Fifth Annual Symposium on Computer
Archtecture",
	Year = 1978,
	Month = "April"}

@inproceedings{k:ada,
	Author = "Randy H. Katz",
	Title = "A Database Approach for Managing {VLSI} Design Data",
	Booktitle = "Proceedings of the 19th Design Automation Conference",
	Month = "June",
	Year = 1982}

@inproceedings{k:net,
	Author = "Randy H. Katz",
	Title = "Network-Attached Storage Systems",
	Booktitle = "Proceedings of the IEEE Scalable High Performance
                     Computing Conference",
	Month = "April",
	Year = 1992,
	Address = "Williamsburg, Virginia",
	Pages = "68 -- 75",
	Annote = "The ultimate goal is to make diskless supercomputers
                  possible.  This paper is basically a review of how
                  I/O and network interfaces work.  The new work proposed
                  is to attach a RAID to a special X-Bus board, whose 
                  purpose is to provide a high bandwidth (40 Mbyte/sec)
                  connection between network and disks.  The technique
                  is to use special memory and connections to eliminate/speed up
                  memory copies in the interfaces."}
	

@inproceedings{kewps:imp,
	Author = "R. H. Katz and S. J. Eggers and D. A. Wood and
	          C. L. Perkins and R. G. Sheldon",
	Title = "Implementing a Cache Consistency Protocol",
	Booktitle = "Proceedings of the Twelfth International
	             Symposium on Computer Architecture",
	Month = "June",
	Year = "1985",
	Address = "Boston, Massachusetts",
	Pages = "276--283",
	Annote = "Proposed ``Berkeley'' coherence mechanism.  See
	          paper by Baer above."}

@inproceedings{km:acc,
	Author = "Alfons Kemper and Guido Moerkotte",
	Title = "Access Support in Object Bases",
	Booktitle = "Proceedings of the ACM SIGMOD International
                     Conference on Management of Data",
	Month = "May",
	Year = 1990,
	Address = "Atlantic City, New Jersey",
	Pages = "364--374",
	Annote = "The authors propose maintaining support relations, 
                  which are relations of tuples of oid's designed to
                  speed path following and associative access.  In order
                  to decide which support relations to use, they provide
                  an analytic model describing the probable sizes, fanouts,
                  etc. based on type declarations.  This, coupled with 
                  a query mix, lets them choose a good set of support
                  relations."} 

@article{kty:que,
	Author = "L. Herschberg and P. D. Ting and S. B. Yao",
	Title = "Query Optimization in star computer networks",
	Journal = "ACM Transactions on Database Systems",
	Volume = 7,
	Number = 4,
	Month = "December",
	Year = 1982,
	Annote = "Apparently gives the basic formula that the size of
		  a join is the sum over all i of $n_im_i$, where
		  $n_i$ is the number of tuples with constant $i$ in
		  the join column of $R$, and $m_i$ is the number of
		  tuples with constant $i$ in the join column of $S$."}

@inproceedings{kk:amo,
	Author = "N. Kamel and R. King",
	Title = "A model of data distribution based on texture analysis",
	Booktitle = "Proceedings of the International SIGMOD Conference",
	Year = 1985,
	Month = "May",
	Address = "Austin, Texas",
	Pages = "319--325",
	Annote = "Apparently use pattern recognition techniques to determine
		  widths of histograms for data distribution."}

@inproceedings{kc:obj,
	Author = "Setrag N. Khoshafian and George P. Copeland",
	Title = "Object Identity",
	Booktitle = "OOPSLA Proceedings",
	Year = 1986,
	Pages = "406--416",
	Annote = "A bunch of stuff about object identity,
	          arguing that trouble arises if one doesn't
		  have ``strong'' identity, where identity
		  is separate from value or addressability."}

@unpublished{kv:sha,
	Author = "Setrag Khoshafian and Patrick Valduriez",
	Title =	 "Sharing, Persistence, and Object Orientation:
		  A Database Perspective",
	Year = 1987,
	Note = "Unpublished Manuscript, MCC",
	Annote = "Describes how sharing, persistence, and
	          object orientation are used in db and pl
		  communities."}

@inproceedings{kk:job,
	Author = "Brent A. Kingsbury and John T. Kline",
	Title = "Job and Process Recovery in a {UNIX}-based Operating
                 System",
	Booktitle = "Proceedings of the Winter USENIX Technical Conference",
	Year = 1989,
	Month = "January",
	Address = "San Diego, California",
	Pages = "355--364",
	Annote = "Describes a checkpointing  and recovery scheme for
                  UNICOS, the Cray operating system.  As the author's
                  say, it turned out to be simple in concept but 
                  difficult to implement.  The idea is to freeze
                  groups of cooperation processes, by waiting for
                  fast I/O to terminate.  Then take a snapshot,
                  which may include a fair amount of kernel state.
                  The goal here is primarly recovery after system
                  shutdown.  No mension of efficiency of checkpointing
                  is given."}

@article{ktm:app,
	Author = "M. Kitsuregawa and H. Tanaka and T. Moto-oka",
	Title = "Application of Hash to Data Base Machine
	         and its Architecture",
	Journal = "New Generation Computing",
	Volume = 1,
	Number = 1,
	Year = 1983,
	Annote = "Not read.  Apparently introduced GRACE algorithm."} 

@inproceedings{ko:buc,
    author="Masaru Kitsuregawa and Yasushi Ogawa",
    title="Bucket Spreading Parallel Hash:  A New, Robust, Parallel Hash
           Join Method for Data Skew in the {S}uper {D}atabase {C}omputer
           ({SDC})",
    booktitle="Proceedings of the Sixteenth International Conference on
               Very Large Data Bases",
    year="1990",
    month="August",
    address="Brisbane, England"}


@phdthesis{k:the,
	Author = "R. Kooi",
	Title = "The optimization of queries in relational database systems",
	School = "Case Western University",
	Address = "Cleveland, Ohio",
	Year = 1980}

@article{ka:imp,
	Author = "K.~G.~Kulkarni and M.~P.~Atkinson",
	Title = "Implementing an Extended Functional Data Model
		 Using {PS}-algol",
	Journal = "Software---Practice and Experience",
	Voume = 17,
	Number = 3,
	Pages = "171--185",
	Month = "March",
	Year = 1987,
	Annote = "Describes an implementation of EFDM, a functional
		  data model proposed by Shipman.  The basic object
		  is an entity; information assosciated with
		  entities is modelled as functions mapping
		  entities to entities.  The query language
		  is an extension of the DAPLEX language ---
		  extended in that they hope to make their
		  language a general purpose programming language.
		  The functional model allows concise queries,
		  since nexting of functions replace explicit
		  joins.
		  
		  Each entity has a unique id.  Associated with
		  an entity is a record containing the values
		  of all functions associated with the entity 
		  (by unique id's if the function is  1-1, lists
		  of id's if it's 1-many.).
		  
		  The implementation was in PS-algol, a language
		  that allows for persistent objects between 
		  program runs and provides transactions on
		  these objects.  No details of the storage
		  or accessing mechanisms for this language
		  were discussed in this paper."}

@article{ks:the,
	Author = "Akhil Kumar and Michael Stonebraker",
	Title = "The Effect of Join Selectivities on Optimal 
		 Nesting Order",
	Journal = "SIGMOD Record",
	Volume = 16,
	Number = 1,
	Month = "March",
	Year = 1987,
	Pages = "28--41",
	Annote = "Their main conclusion is that the relative cost
		  of plans remain relatively constant over a wide
		  range of join selectivities.  However, they make
		  a bunch of simplifying assumptions.  The main one 
		  is that they assume that one selectivity characterizes 
		  the whole join."}

@inproceedings{ks:per,
	Author = "Akhil Kumar and Michael Stonebraker",
	Title = "Performance Evaluation of an Operating
	         System Transaction Manager",
	Booktitle = "Proceedings of the Thirteenth International
	             Conference on Very Large Databases",
	Year = 1987,
	Pages = "473--481",
	Annote = "Through a lot of simulation, the authors' conclude
	          that an OS transaction manager is a big lose
		  when compared to leaving the manager in the
		  DB.  This is because the DB has lots of
		  semantic info about the objects it's managing ---
		  log pages, index structures, etc --- so it can
		  lock much smarter."}

@inproceedings{kp:the,
	Author = "Manoj Kumar and Gregory F. Pfister",
	Title = "The Onset of Hot Spot Contention",
	Booktitle = "Proceedings of the International Conference on
		     Parallel Processing",
	Pages = "28-34",
	Year = 1986,
	Annote = "Desribes tree blocking problem."}

@inproceedings{kl:sys,
	Author = "H. T. Kung and Philip L. Lehman",
	Title = "Systolic Arrays for Relational Database Operations",
	Booktitle = "Proceedings of the ACM SIGMOD International
	             Conference on Management of Data",
	Year = 1980,
	Pages = "105--116",
	Annote = "I think the problem with this stuff is that
	          actually performing the operations this hardware
		  supports is not the bottleneck."}

@article{lls:asp,
	Author = "Herman Lam and Chiang Lee and Stanley Y. W. Su",
	Title = "A Special Function Unit for Database Operations {SFU-DB}:
		Design and Performance Evaluation",
	Journal = "IEEE Transactions on Computers",
	Volume = 40,
	Number = 3,
	Month = "March",
	Year = 1991,
	Pages = "263 -- 275",
	Annote = "A hardware radix sort (byte by byte.)  They note that
		the sorter will sort $n$ keys of $m$ bytes each in
		$O(nm)$, and if you assume $m$ is constant, this is $O(m)$.
		They justify their assumption that $m$ is constant by
		pointing out that this assumption is made in the standard
		sorting literature (comparsisons take linear time.)  To me 
		this seems like cheating - in comparison sorting, accomodating
		different length keys adds time to the comparison operator.  Here,
		adding different length keys increases the number of passes
		through the algorithm."}

@article{ly:eff,
	Author = "M. Seetha Lakshmi and Philip S. Yu",
	Title = "Effectiveness of Parallel Joins",
	Journal = "IEEE Transactions on Knowledge and Data Engineering",
	Volume = 2,
	Number = 4,
	Month = "December",
	Year = 1990,
	Annote = "Studies skew, both due to skew in attribute values
		 and 'stochastic skew' due to stochastic variations
	         in processing times at each node.  Conclusion: skew
		 causes problems if there are lots of processors."} 

@article{llow:the,
	Author = "C. Lamb and G. Landis and J. Orenstein and D. Weinreb",
	Title = "The {ObjectStore} Database System",
	Journal = "Communications of the ACM",
	Volume = 34,
	Number = 10,
	Month = "October",
	Year = 1991}


@unpublished{l:aqu,
	Author = "Keith A. Lantz",
	Title = "A Quasi-Annotated Bibliography on Systems",
	Note = "Unpublished Memorandum, Stanford University",
	Year = 1984}

@article{llaffv:the,
	Author = "Edward D. Lazowska and Henry M. Levy and
	          Guy T. Almes and Michael J. Fischer and
		  Robert J. Fowler and Stephen C. Vestal",
	Title = "The Architecture of the Eden System",
	Journal = "Some ACM Journal",
	Month = "Whoknows",
	Year = "????",
	Annote = "Has a good object-system bibliography."}

@inproceedings{lg:lan,
	Author = "Insup Lee and Vijay Gehlot",
	Title = "Language Constructs for Distributed Real-Time Programming",
	Booktitle = "Proceedings of the IEEE Real-Time Systems Symposium",
	Year = 1985,
	Month = "December",
	Address = "San Diego, California",
	Pages = "57--66",
	Annote = "Good bibliography.  Design goals are 1. language
		  constructs that can be used to specify timing
                  constraints of code execution and interprocess 
		  communication; 2. Detection and handling of exceptions
                  caused by timing constaint violations at run-time;
                  3.  run-time support system can use timing info
                  specified within process for scheduling.  Three kinds
                  of timing constraints are useful: maximum, minimum,
                  and duration.  These constraints are denoted using
                  temporal intervals, which are deadline, minimum delay,
		  maximum delay, maximum execution time, and maximum
                  elapse time.  General construct is 
			start <d-part> [ <e-part>][<dl-part>] do
			 	<start-body>
			[ <exceptions> ]
			end
		  where
			<d-part> ::= now | at <abs-time> | after <rel-time>,
			<e-part> ::= execute <rel-time> | elapse <rel-time>,
			<dl-part ::= by <abs-time> | within <rel-time>"}

@article{lldhns:the,
	Author = "Paul J. Leach and Paul H. Levine and Bryan P. Douros and
		James A. Hamilton and David L. Nelson and Bernard L. Stumpf",
	Title = "The Architecture of an Integrated Local Network",
	Journal = "IEEE Journal on Selected Areas in Communications",
	Volume = "SAC-1",
	Number = 5,
	Month = "November",
	Year = 1983}


@inproceedings{lc:aco,
	Author = "Tobin J. Lehman and Michael J. Carey",
	Title = "A Concurrency Control Algorithm for
                 Memory-Resident Database Systems", 
	Booktitle = "Proc. of the 3rd Int'l. Conf.
        on Foundations of Data Organization and Algorithms", 
	Address = "Paris, France",
        Month = "June",
	Year = 1989}

@inproceedings{lc:que,
	Author = "Tobin J. Lehman and Michael J. Carey",
	Title = "Query Processing in Main Memory Database
		 Management Systems",
	Booktitle = "Proceedings of the ACM-SIGMOD International Conference
		     on Management of Data",
	Year = 1987,
	Month = "May",
	Address = "San Fransisco, California",
	Pages = "239--250",
	Annote = "This paper studies algorithms for selection, join, and 
		  projection in main memory DBMS.  They conclude that
		  the ``T-Tree'' is good for ordered data, while hashing
		  is good for unordered data.  Note that the reason
		  they consider hashing suboptimal is that it typically
		  required about 2.3 words per tuple to store the
		  index whereas T trees required about 1.6.
		  
		  The found that a T-tree based merge join works
		  well, but they only considered hashes in which
		  the T-tree indices already exist and the hash
		  table indices do not, so their analysis of
		  the hash based joins included the overhead
		  of constructing the hash table.  Also
		  found that hashing is best for projections."}

@inproceedings{lc:are,
	Author = "Tobin J. Lehman and Michael J. Carey",
	Title = "A Recovery Algorithm for a High-Performance 
	         Memory-Resident Database System",
	Booktitle = "Proceedings of the ACM-SIGMOD International Conference
		     on Management of Data",
	Year = 1987,
	Month = "May",
	Address = "San Fransisco, California",
	Pages = "104--117"}

@inproceedings{lsz:ase,
	Author = "H. O. Leilich and G. Stiege and H. Ch. Zeidler",
	Title = "A Search Processor for Database Management Systems",
	Booktitle = "Fourth International Conference on Very Large
	             Database Systems",
	Year = 1978}


@inproceedings{lnl:imp,
        Author = "Kwei-Jay Lin and Swaminathan Natarajan and Jane W.-S. Liu",
        Title = "Imprecise Results: Utilizing Partial Computations
                 in Real-Time Systems",
        Booktitle = "Proceedings of the IEEE Real-Time Systems Symposium",
        Month = "December",
        Year = 1987,
        Address = "San Jose, California",
        Pages = "210--217",
        Annote = "The idea is that often a job must be terminated because
                  of a deadline, and perhaps partial results are of some
                  utility and shouldn't be thrown away.  They identify
                  two types of useful partial results: (1) milestones, in which
                  the result of a computation gets more accurate as time
                  progresses, and (2) sieve,  in which processing can be
                  broken into stages such that each refines the next and
                  non are absolutely necessary."}

@article{lss:the,
	Author = "S. C. Lin and D. C. P. Smith and J. M. Smith",
	Title = "The Design of a Rotating Associative Memory for
	         Relational Database Applications",
	Journal = "ACM Transactions on Database Systems",
	Volume = 1,
	Number = 1,
	Pages = "53--75",
	Year = 1976,
	Month = "March"}


@article{lhmwy:com,
	Author = "Bruce G. Lindsay and Laura M. Haas and C. Mohan and
	          Paul F. Wilms and Robert A. Yost",
	Title = "Computation and Communication in {R*}: A Distributed
	         Database Manager",
	Journal = "ACM Transactions on Computer Systems",
	Volume = 2,
	Number = 1,
	Month = "February",
	Year = "1984",
	Pages = "24--38",
	Annote = "An R* computation consists of a tree of processes
		  connected by virtual circuits.  The process
	          management and communication protocols provide
		  reliable transactions.  Remote client processes
		  exist for the duration of the transaction."}

@phdthesis{l:sha,
	Author = "Kai Li",
	Title = "Shared Virtual Memory on Loosely Coupled 
	         Multiprocessors",
	School = "Yale University",
	Month = "September",
	Number = "YALEU-RR-492",
	Year = 1986}


@misc{l:per,
	Author = "Richard J. Lipton",
	Year = "1988",
	Note = "personal communication"}

@article{ls:gua,
	Author = "Barbara H. Liskov and Robert W. Scheifler",
	Title = "Guardians and Actions: Linguistic Support for
		 Robust, Distributed Programs",
	Journal = "ACM Transactions on Programming Languages and Systems",
	Volume = 5,
	Number = 3,
	Pages = "381--404",
	Month = "July",
	Year = 1983}

@inproceedings{lkb:declus,
	Author = "M. Livny and S. Khoshafian and H. Boral",
	Title = "Multi-disk management algorithms",
	Booktitle = "Proceedings of 1987 SIGMETRICS Conf.",
	Month = "May",
	Year = 1987}

@inproceedings{l:gra,
	Author = "G. M. Lohman",
	Title = "Grammar-Like Functional Rulse for Representing
		 Query Optimization Alternatives",
	Booktitle = "Proceedings of the ACM SIGMOD Conference",
	Address = "Chicago, Illinois",
	Month = "June",
	Year = 1988,
	Pages = "18--27"}

@inproceedings{lfl:imp,
	Author = "M. Lee and J. Freytag and G. Lohman",
	Title = "Implementing an Interpreter for Functional Rules
                 in a Query Optimizer",
	Booktitle = "Proceedings of the Fourteenth International
		     VLDB Conference",
	Year = 1988,
	Month = "August",
	Pages = "218--229",
	Address = "Los Angeles, California"}

@inproceedings{ly:alo,
	Author = "Raymond A. Lorie and Honesty C. Young",
	Title = "A Low Communication Sort Algorithm for a Parallel
		 Database Machine",
	Booktitle = "Proceedings of the Fifteenth International Conference
		     on Very Large Databases",
	Month = "August",
	Year = "1989",
	Address = "Amsterdam, The Netherlands",
	Pages = "125--134",
	Annote = "An algorithm for n-input, single-output sorting.  Uses
		partitioning; the partitioning elements are chosen
		by first sorting the original partitions, then doing some
		magic.  In more detail, the algorithm works by 1)
		everyone sorts their own local partition, then 2) sample
                and find global split elements, then 3) merge incoming
                sorted runs.  Step 2) is never discussed in any detail;
		furthermore, in their analytically derived data, they
		assume that the split elements are found exactly and
		at zero cost."}

@inproceedings{l:new,
	Author = "H. Lu",
	Title = "New Strategies for Computing the Transitive Closure of a
	         Database Relation",
	Booktitle = "Proceedings of the Thirteenth International
                     Conference on Very Large Databases",
	Pages = "267--274",
	Year = 1987,
	Month = "September",
	Address = "Brighton, England"}

@inproceedings{lst:opt,
	Author = "Hongjun Lu and Ming-Chien Shan and Kian-Lee Tan",
	Title = "Optimization of Multi-Way Join Queries for 
		Parallel Execution",
	Booktitle = "Proceedings of the Seventeenth International Conference
		on Very Large Data Bases",
	Month = "August",
	Year = 1991,
	Pages = "????",
	Address = "Barcelona, Spain",
	Annote = "
		Considers join order and processor allocation for 
		multi-way joins on shared-memory multiprocessors.
		They consider only {\em syncrhonized} QEPs, that is,
		those in which the execution is divided into phases,
		each phase possibly consisting of many joins running
		in parallel.  Between phases, temporary results are
		written to disk.  The query evaluation is synchronized
		at the boundaries between phases.
"}

@techreport{lt:ady,
	Author = "Hongjun Lu and Kian-Lee Tan",
	Title = "A Dynamic Load-Balanced Task-Oriented Approach to
		 Parallel Query Processing",
	Institution = "Department of Information Systems and Computer Science,
	               National University of Singapore",
	Month = "July",
	Year = 1991,
	Number = "Discs Publication TRC7/91",
	Annote = "The basic idea is to handle skew by (1) finding 
		statistics about the relations by scanning, then
		(2) defining a bunch of small tasks, then
		(3) load-balance by first making an approximate
		allocation of work to processors, then doing load
		stealing once the join is running.  The paper is
		essentially Wolf, Yu, and Dias with dynamic
		rather than static task allocation.  Their analytic
		modeling shows that the method is much better than
		running the join without load balancing."}


@inproceedings{l:sel,
	Author = "Clifford A. Lynch",
	Title = "Selectivity Estimation and Query Optimization in Large
		 Databases with Highly Skewed Distributions of Column
		 Values",
	Booktitle = "Proceedings of the Fourteenth International
                     Conference on Very Large Databases",
	Address = "Los Angeles, California",
	Month = "August",
	Year = 1988,
	Pages = "240--251",
	Annote = "Another paper dealing with selectivities.  Here the
		  main focus is on the selectivity of ``attr = const''
		  queries.  He first points out that in a bibliographic
		  database, uniform distributions are radically bad.
		  Next, shows that Zipf isn't much better.  Finally,
		  he proposes that user-supplied estimators that use
		  domain knowledge should be used.  

		  He has some interesting comments on ``non-parametric''
		  methods, e.g., histograms.  They require ``local
		  smoothness'' for equality selections to work;
		  note that this isn't the case for ``less-than''
		  selectivities."}

@inproceedings{ml:rop,
	Author = "Lothar F. Mackert and Guy M. Lohman",
	Title = "{R*} Optimizer Validation and Performance Evaluation
		 for Distributed Queries",
	Booktitle = "Proceedings of the Twelfth International Conference
		     on Very Large Databases",
	Address = "Kyoto, Japan",
	Month = "August",
	Pages = "149--159",
	Year = 1986,
	Annote = "Among other things, notes that when shipping
		  a relation they can't ship indices, because
		  indices refer to addresses. "}

@inproceedings{m:the,
	Author = "S. E. Madnick",
	Title = "The {Infoplex} Database Computer: Concepts and
		 Directions",
	Booktitle = "IEEE Computer Conference",
	Year = 1979,
	Month = "February"}

@article{mcs:sta,
	Author = "Michael V. Mannino and Paicheng Chu and Thomas Sager",
	Title = "Statistical Profile Estimation in Database Systems",
	Journal = "Computing Surveys",
	Month = "September",
	Year = 1988,
	Volume = 20,
	Number = 3,
	Pages = "191--221",
	Annote = "A survey of estimation techniques.  It doesn't seem
		  to say much.  Almost no attention is given to 
		  error bounds or confidence intervals --- what
		  does appear is bounds on ``worst case'' errors.
		  Good bibliography."}

@inproceedings{m:ast,
	Author = "Jai Menon",
	Title = "A Study of Sort Algorithms for Multiprocessor
	         Database Machines",
	Booktitle = "Proceedings of the Twelfth International
		     Conference on Very Large Databases",
	Pages = "197--206",
	Year = 1986,
	Annote = "Concludes that modified block bitonic sort
	          algorithms are best over wide range
		  of machines.  Analysis is based on extremely
		  simple model --- a contention free shared
		  pool of disks."}

@inproceedings{md:the,
	Author = "Dennis R. McCarthy and Umeshwar Dayal",
	Title = "The Architecture of an Active Data Base
		 Management System",
	Booktitle = "Proceedings of the International SIGMOD
		     Conference",
	Year = 1989,
	Location = "Portland, Oregon",
	Month = "May",
	Pages = "215--224",
	Annote = "
		An active DBMS is one that automatically
		executes specified actions when specific
		conditions arise.  HiPAC proposes Event-
		Condition-Action rules as a formalism for
		active database capabilities.  The event
		causes HiPAC to evaluate the rule's condition.
		The condition is a colleciton of queries.  The
		action is executed when the rule is triggered
		and its condition is satisfied.  E-C coupling
		specifies when the condition is evaluated
		relative to the transaction in which the triggering
		event is signalled; C-A coupling is analagous.
		The coupling can either be immediate (in subtransaction 
		of transaction of as triggering event), or separate
		(in a concurrent top-level separate transaction), 
		or deferred (in subtransaction of
		transaction for triggering event, but
		just before top-level transaction commits.)

		Primitive events are either Database Operations,
		or Temporal Events, or External Notification.  These
		can be combined using disjunction and sequence
		operators.

		HiPAC uses a nested-transaction model.  Briefly,
		top-level transactions are atomic, serializable, and
		permanent.  Concurrently executing sibling transactions
		are serializable.  The effects of a subtransaction
		do not become permanent until all of its ancestors
		commit.

		If more than one rule fires, there is no conflict
		resolution --- the subtransactions created execute
		concurrently, with serializability guaranteed by
		the transaction manager.

		Applications signal HiPAC, and HiPAC can in turn
		call applications in response to events.  This
		allows interprocess communication through rule
		firings.  In prototype applications, applications
		tend to be simple, with control logic specified by
		the rules.

		Flow of control is as follows: the Event Detector
		detects an event, and notifies the rule manager.
		The rule manager determines which rule to fire,
		then schedules condition evaluation
		based on coupling modes.  Then the rule manager
		calls on the transaction manager to create a
		transaction in which to execute the condition,
		and if it is satisfied, calls on the transaction
		manager to create a transaction for executing the
		action."}	

@techreport{mnss:sec,
	Author = "S. P. Miller and B. C. Neuman and J. I. Schiller and
                  J. H. Saltzer",
	Title = "Section {E.2.1}: Kerberos Authentication and Authorization
                 System",
	Number = "Project Athena Technical Plan",
	Institution = "M.I.T. Project Athena",
	Address = "Cambridge, MA",
	Month = "December",
	Year = 1987}

@inproceedings{m:rot,
	Author = "N. Minsky",
	Title = "Rotating Storage Devices as Partially Associative
	         Memories",
	Booktitle = "Fall Joint Computer Conference",
	Year = 1972}

@inproceedings{m:ano,
	Author = "M. Missikoff",
	Title = "An Overview of the project {DBMAC} for a 
	         relational machine",
	Booktitle = "Sixth Workshop on Computer Architecture for 
	             Non-numerical Processing",
	Month = "June",
	Year = 1981}


@techreport{ms:evo,
	Author = "P. R. McJones and G. F. Swart",
	Title = "Evolving the {UNIX} System Interface to Support Multithreaded
		 Programs",
	Institution = "DEC Systems Research Center",
	Month = "September",
	Year = 1987,
	Number = 21}

@inproceedings{ml:eff,
	Author = "C. Mohan and B. Lindsay",
	Title =  "Efficient commit protocols for the tree of 
		processes model of distributed transactions",
	Booktitle = "Proc. ACM Symp. on Principles of Distributed Computing" ,
	Month = "August",
	Year = 1983,
	Address = "Montreal, Canada",
	Pages = "76 -- 80"}



@article{mhlps:ari,
	Author = "C. Mohan and D. Haderle and B. Lindsay and
		  H. Pirahesh and P. Schwarz",
	Title = "{ARIES}: A Transaction Recovery Method Supporting
		 Fine-Granularity Locking and Partial Rollbacks
		 Using Write-Ahead Logging",
	Journal = "ACM Transactions on Database Systems",
	Month = "March",
	Year = 1992}

}

@inproceedings{m:the,
	Author = "A. K. Mok",
	Title = "The Design of Real-Time Programming Systems Based on
		 Process Models",
	Booktitle = "Proceedings of the IEEE Real-Time Systems Symposium",
	Month = "December",
	Year = 1984,
	Address = "Austin, Texas",
	Pages = "5--17",
	Annote = "Gives a number of basic results about scheduling
		  in hard real-time environments, assuming both
		  periodic and sporadic processes and total 
		  knowledge about the run times of the processes.
		  For example, shows
		  1. If the run-time scheduler can interrupt a running
		     process at any time, and the processes are 
                     independent, then least slack is an optimal
		     on-line scheduler.  (Optimal = finds a feasible
		     schedule if one exists; on-line means scheduling 
		     decisions do not depend on knoledge of future
		     requests)
		  2. When there are mutual exclusion constraints, it
		     is impossible to find a totally on-line optimal
		     run-time scheduler.
		  3. Deciding if a set of periodic processes that
                     interact only by using semaphores for mutual
                     exclusion can be scheduled is NP-Hard.

		  The interesting point is that interactions between
                  processes make things much harder."}

@article{mdl:the,
	Author = "A. Montgomery and D. D'Souza and S. Lee",
	Title = "The cost of relational algebraic operations on skewed
		 data: Estimates and experiments",
	Journal = "Information Processing Letters",
	Year = 1983,
	Pages = "235--241",
	Annote = "Investigates what happens when you use the
                  uniformity assumption on skewed data.  They 
                  found that it overestimates selections,
                  and underestimates joins." }



@article{mschrs:and,
	Author = "J. H. Morris and M. Satyanarayanan and M. H.
		  Conner and J. H. Howard and D. S. H. Rosenthal
		  and F. D. Smith",
	Title = "Andrew: A Distributed Personal Computing
		 Environment",
	Journal = "Communications of the ACM",
	Volume = 29,
	Number = 3,
	Pages = "184--201",
	Month = "March",
	Year = 1986}

@incollection{ms:man,
	Author = "{J. Eliot B.} Moss and Steven Sinofsky",
	Title = "Managing Persistent Data with Mneme: Designing a
		 Reliable, Shared Object Interface",
	Booktitle = "Lecture Notes in Computer Science: Advances in 
		     Object-Oriented Database Systems",
	Volume = 334,
	Publisher = "Springer-Verlag",
	Pages = "298--316",
	Month = "September",
	Year = 1988,
	Annote = "Good overview of Mneme's goals."}

@inproceedings{m:add,
	Author = "{J. Eliot B.} Moss",
	Title = "Addressing Large Distributed Collections of Persistent
		 Objects: The Mneme Project's Approach",
	Booktitle = "Proceedings of the Second International Workshop
		     on Database Programming Languages",
	Address = "Gleneden Beach, Oregon",
	Month = "June",
	Year = 1989,
	Pages = "269--285",
	Annote = "Argues against uniform object id spaces and 
                  address spaces.  Presents Mneme's approach to
		  the problem."}



@inproceedings{md:equ,
	Author = "M. Muralikrishna and D. DeWitt",
	Title = "Equidepth histograms for  estimating selectivity
		 factors for multi-dimensional queries",
	Booktitle = "Proceedings of the International SIGMOD Conference",
	Year = 1988,
	Month = "June",
	Address = "Chicago, Illinois",
	Pages = "28--36"}

@techreport{mk:add,
	Author = "B. Muthuswamy and G. Kerschberg",
	Title = "A {DDSM} for relational query optimization",
	Institution = "University of South Carolina, Columbia",
	Year = 1985,
	Note = "As cited in~\cite{mcs:sta}.",
	Annote = "Another formula for estimating joins from histograms."}

@inproceedings{lrv:o2a,
	Author = "C. Lecluse and P. Richard and F. Velez",
	Title = "O2, an Object-Oriented Data Model",
	Booktitle = "Proceedings of the ACM SIGMOD Conference",
	Address = "Chicago, Illinois",
	Month = "June",
	Year = "1988",
	Pages = "424--433"} 


@inproceedings{lh:mem,
	Author = "Kai Li and Paul Hudak",
	Title = "Memory Coherence in Shared Virtual Memory Systems",
	Booktitle = "Proceedings of  the Fifth SIGACT-SIGOPS Symposium
		     on Principles of Distributed Computing",
	Month = "August",
	Year = 1986,
	Pages = "1--11",
	Annote = "Investigates several algorithms for memory coherence
		  in SVM systems.  The conclusion is that the dynamic 
		  distributed manager algorithm seems to be the best.
		  (As compared to centralized or fixed distributed
		   managers.) Briefly, it works as follows: each processor
		   records a triple for each page in its memory.  
		   ({\em prob\_owner} is the process with write access
		   to the page, or the first in a linked list that
		   will terminate in that processor.  {\em copy\_set}
		   is a list of all processors with (read) copies, and
		   is maintained by th owner.  {\em lock} is used for
		   synchronization of requests.)   An expermental 
		   implementation on a ring of Apollo workstations
		   got good results for problems with a high degree
		   of locality."}

@techreport{ln:mul,
	Author = "Kai Li and Jeffrey F. Naughton",
	Title = "Multiprocessor Main Memory Transaction Processing",
	Institution = "Princeton University",
	Year = 1988,
	Month = "June",
	Number = "CS-TR-159-88"}

@inproceedings{ln:mulproc,
	Author = "Kai Li and Jeffrey F. Naughton",
	Title = "Multiprocessor Main Memory Transaction Processing",
	Booktitle = "Proceedings of the International Symposium on Databases in Parallel and Distributed Systems",
	Month = "December",
	Pages = "177--189",
	Address = "Austin, Texas",
	Year = 1988}

@unpublished{ln:ana,
	Author = "Kai Li and Jeffrey F. Naughton",
	Title = "Analysis of a Multiprocessor Main Memory Transaction Processing System",
	Institution = "Princeton University",
	Note = "Submitted for publication.",
	Year = 1989}

@inproceedings{lnp:are,
	Author = "Kai Li and Jeffrey F. Naughton and James S. Plank",
	Title = "A Real-Time, Concurrent Checkpoint and Recovery
		 Algorithm for Parallel Programs",
	Booktitle = "Proceedings of the 1990 ACM Symposium on Principles
		     and Practice of Parallel Programming",
	Month = "March",
	Year = 1990,
	Address = "Seattle, Washington"}

@inproceedings{lnp:che,
	Author = "Kai Li and Jeffrey F. Naughton and James S. Plank",
	Title = "Checkpointing Multicomputer Programs",
	Booktitle = "Proceedings of the 1991 IEEE Symposium on
		Reliable Distributed Systems",
	Month = "September",
	Year = 1991,
	Address = "Pisa, Italy",
	Note = "To appear."}

@article{lnp:ane,
	Author = "Kai Li and Jeffrey F. Naughton and James S. Plank",
	Title = "An efficient checkpointing method for multicomputers
                 with wormhole routing",
	Journal = "International Journal of Parallel Processing",
	Volume = 20,
	Number = 3,
	Month = "June",
	Year = 1992}

@inproceedings{ln:exp,
        Author = "Kwei-Jay Lin and Swaminathan Natarajan",
        Title = "Expressing and Maintaining Timing Constraints in
                 {FLEX}",
        Booktitle = "Proceedings of the IEEE Real-Time Systems Symposium",
        Month = "December",
        Year = 1988,
        Address = "Huntsville, Alabama",
        Pages = "96--105",
        Annote = "A constraint in FLEX has the syntax
                  [label:] bool-exp [~> exception_handler;] {CB}
                  The meaning of this is: if boolean expression
                  is true, do the exception handler; otherwise do
                  the contraint block CB.  Every CB has the following
                  attributes defined: start, finish, duration, period,
                  and priority.  Constraints can also be defined on
                  resourses, e.g. Memory.avail>low ~> doSomething.
                  The point is that these constraints are first class
                  objects, and that programs can use them to govern
                  their behavior, not just the system scheduler.
                  This meshes well with their imprecise results
		  paradigm.  Some examples:

		  C1: distance < 1 ~> react(object); 
			foreach object do
				get_distance(object);

		  C2: (finish < (last+10)) && (priority >= 2) ...

		  J2: (duration <(5-J1.duraction)) && (duration < 4) ..."}

@techreport{lhmpw:asn,
	Author = "Bruce Lindsay and Laura Haas and C.~Mohan
		  and Hamid Pirahesh and Paul Wilms",
	Title = "A Snapshot Differential Refresh Algorithm",
	Institution = "IBM Almaden Research Center", 
	Number = "RJ 4992 (52169)",
	Month = "January",
	Year = "1986",
	Annote = "A snapshot is useful if an application has to
		  work with a frozen, consistent state of the
		  database.  The challenge addressed here is
		  how to take such a snapshot without copying
		  the entire database.  The main insight seems
		  to be defining regions of uninteresting
		  contiguous sections of the database tables."}



@article{nhs:the,
	Author = "J. Nievergelt and H. Hinterberger and K. C. Sevcik",
	Title = "The Grid File: An Adaptable, Symmetric Multikey
		 File Structure",
	Journal = "ACM Transactions on Database Systems",
	Volume = 9,
	Number = 1,
	Pages = "38--71",
	Year = 1984}

@article{ol:has,
	Author = "Edward R. Omiecinski and Eileen Tien Lin",
	Title = "Hash-Based and Index-Based Join Algorithms for
                 Cube and Ring Connected Multicomputers",
	Journal = "IEEE Transactions on Knowledge and Data 
                   Engineering",
	Volume = 1,
	Number = 3,
	Month = "September",
	Year = 1989,
	Pages = "329--343",
	Annote = "Considers hybrid hash vs. join index parallel
                  join algorithms.  Considers network topology
                  as a big performance determinant.  Found that
                  hybrid hash won almost everywhere (unless
                  join selectivity was very high, e.g. very
                  small result.)  Basic idea was to maintain 
                  two fragments of join index at each site: one
                  clustered on R and containing the entries for
                  the R tuples at the site, the other for S."}

@inproceedings{o:per,
    author="Edward Omiecinski",
    title="Performance analysis of a load balancing hash-join algorithm
           for a shared memory multiprocessor",
    booktitle="Proceedings of the Seventeenth International Conference on
               Very Large Data Bases",
    year="1991",
    month="September",
    address="Barcelona, Spain"}

@inproceedings{ov:bui,
	Author = "Peter {van Oosterom} and Tom Vijlbrief",
	Title  = "Building a {GIS} on top of the open {DBMS} {Postgres}",
	Booktitle = "Proceedings of EGIS",
	Year = 1991,
	Month = "April",
	Address = "Brussels, Belgium",
	Annote = "First paper on GEO.  Mostly talks about how
                  GEO makes use of the extensible features of
                  Postgres.  The main point seems to be that
                  Postgres allows them to store all of the
                  data within the DBMS, as opposed to the 
                  more common dual architecture in which an RDBMS
                  stores the alphanumeric data and special purpose
                  storage system stores and processes the spatial 
                  data."}


@book{o:the,
	Author = "E. Organick",
	Title = "The {MULTICS} System",
	Publisher = "MIT Press",
	Address = "Cambridge, Massachusetts",
	Year = 1972}

@article{od:bea,
	Author = "John Ousterhout and Fred Douglis",
	Title = "Beating the I/O Bottleneck: A Case For Log-Structured
                 File Systems",
	Journal = "SIGOPS Newletter",
	Year = 1989,
	Volume = "?",
	Number = "?",
	Annote = "Argues that with huge disk caches, the disk traffic
                  due to reads is essentially nil, so the best way
                  to reduce the write disk traffic is to write files
                  contiguously to a log on disk, using the log only for
                  recovery after crashes.  Says that the problem with
                  vanilla disk caching is too much write traffic, while
                  the problem with battery backed caching is recovery
                  on OS failures."}
                   

@inproceedings{oss:rap,
	Author = "E. A. Ozkarahan and S. A. Schuster and K.C. Smith",
	Title = "{RAP} --- Associative Processor for Database
                 Management",
	Booktitle = "AFIPS",
	Volume = 44,
	Year = 1975,
	Pages = "379--388"}

@article{oss:per,
	Author = "E. A. Ozkarahan and S. A. Schuster and K. C. Sevcik",
	Title = "Performance Evaluation of a Relational Associative
	         Processor",
	Journal = "ACM Transactions on Database Systems",
	Volume = 2,
	Number = 2,
	Year = 1977,
	Month = "June"}

@inproceedings{pl:sup,
	Author = "Douglas Z. Pan and Mark A. Linton",
	Title = "Supporting Reverse Execution of Parallel Programs",
	Booktitle = "Proceedings of the ACM Workshop on Parallel and 
		     Distributed Debugging",
	Month = "May",
	Year = 1988,
	Address = "Madison, Wisconsin",
	Annote = "The main idea is to support tracing and replaying of
		  a single process within a parallel program.  To do
		  so, the compiler inserts special code for any
		  IPC (messages or shared memory ops).  During forward
		  execution, all IPC results in log writes; during
                  replay, all IPC is done through reading the log.
                  To speed up replay, checkpoints are taken periodically
                  by forking and suspending a process (this suspended
                  process checkpoints the process state at the
                  time of the checkpoint.)

		  Some differences with Li, Naughton, and Plank's work:
		  1) Checkpoint here means being able to restart during
                     debugging, and makes no guarantees about what happens
                     after a crash.
		  2) This scheme requires compiler modification.
                  3) This scheme degrades rapidly as the percentage of
                     shared memory reads/writes increases.  (One log
                     write per shared memory read, for example.)
                  4) This scheme is a per-process (thread) scheme, and
                     does not checkpoint the whole computation.
                  5) This scheme handles programs with I/O, and seeks to
                     achieve exact replay (we just get resumption, but
                     no guarantee that after the computatin resumes it
                     will behave as it did originally.)"} 

@article{pp:alo,
	Author = "Mark S. Papamarcos and Janak H. Patel",
	Title = "A low-overhead coherence solution for multiprocessors
	         with private cache memories",
	Booktitle = "Proceedings of the Eleventh Annual International
                     Symposium on Computer Architecture",
	Month = "June",
	Year = 1984,
	Address = "Ann Arbor, Michigan",
	Pages = "348--354",
	Annote = "Introduced the ``Illinois'' protocol.  See paper
	          by Archibald and Baer above."}

@inproceedings{p:alo,
	Author = "J. L. Parker",
	Title = "A Logic per Track Retrieval System",
	Booktitle  = "IFIP Congress",
	Year = 1971}

@inproceedings{p:ahi,
	Author = "B. Parhami",
	Title = "A Highly Parallel Computing System for Information
	         Retrieval",
	Booktitle = "Fall Joint Computer Conference",
	Year = 1972}

@inproceedings{pgk:aca,
	Author = "David A. Patterson and Garth Gibson and Randy H. Katz",
	Title = "A Case for Redundant Arrays of Inexpensive Disks
		({RAID})",
	Booktitle = "Proceedings of the SIGMOD International 
		     Conference on Management of Data",
	Year = 1988,
	Month = "May",
	Location = "Chicago, Illinois",
	Pages = "109--116",
	Annote = "The authors argue that an array of cheap disk offers
	          better performance, lower cost, higher reliability
		  than a single big disk.  They survey 5 schemes of
		  redundancy for reliability, ranging from mirroring
		  to error correcting codes.  Some schemes support 
		  higher I/O throughput, others more I/O's."}

@inproceedings{pn:hot,
	Author = "G. F. Pfister and V. A. Norton",
	Title = "Hot-spot contention and combining in multistage
	         interconnection networks",
	Booktitle = "Proceedings of the International Conference on
	             Parallel Processing",
	Pages = "790--797",
	Year = 1985}

@inproceedings{pbghkmmnw:the,
	Author = "G. F. Pfister and W. C. Brantely and D. A. George
		 and S. L. Harvey and W. J. Kleinfelder and
		 K. P. McAuliffe and E. A. Melton and V. A. Norton and
		 J. Weiss",
	Title = "The {IBM} {Research} {Processor} {Prototype} ({RP3})",
	Booktitle = "Proceedings of the International Conference on Parallel
		    Processing",
	Month = "August",
	Year = 1985}

@inproceedings{pc:acc,
	Author = "Gregory Piatetsky-Shapiro and Charles Connell",
	Title = "Accurate Estimation of the Number of Tuples Satisfying
                 a Condition",
	Booktitle = "Proceedings of the ACM SIGMOD International Conference
	         on Management of Data",
	Address = "Boston, Massachusets",
	Year = 1984,
	Month = "June",
	Pages = "256--276",
	Annote = "Here by ``condition'' they mean things of the form
		  $R.X op const$, where $op$ is among $\{<, >, \ge, \le
		  \ne\}$.  Argues that instead of histograms, which 
		  maintain equal width, should maintain ``distribution
		  steps,'' where buckets have equal height.  Shows how
		  this approach can be much better than System-R's
		  assumption that attributes are uniformly distributed
		  between min and max.  Here the accuracy is bounded
		  by the size of the steps --- the more steps, the
		  better.  Also discusses approximately computing
		  these steps through random sampling, using nonparametric
		  statistics to provide error bounds.  A book to check
		  for this is Dixon and Massey's ``Introduction to 
		  Statistical Analysis.''"}

@inproceedings{pp:pub,
	Author = "Michael L. Powell and David L. Presotto",
	Title = "Publishing: A Relaiable Broadcast Communication Mechanism",
	Booktitle = "Proceedings of the ACM SIGOPS Symposium on Operating
		     System Principles",
	Year = 1983,
	Month = "October",
	Pages = "100--109",
	Annote = "Main idea: have a recorder processor listening to 
		network traffic, logging all messages and checkpoints
                of processes.  Principle difference from multicomputer
		environment: want to recover from partial failures
		without complete shutdown, which implies graceful
		re-integration of recovering processes."}

@inproceedings{p:ont,
	Author = "Calton Pu",
	Title = "On-the-fly, Incremental, Consistent Reading of Entire
		 Databases",
	Booktitle = "Proceedings of the Eleventh International Conference
	             on Very Large Databases",
	Month = "August",
	Year = "1985",
	Address = "Stockholm, Sweden",
	Pages = "369--375"}

@article{q:par,
	Author = "M. J. Quinn",
	Title = "Parallel Sorting Algorithms for Tightly Coupled Multiprocessors",
	Journal = "Parallel Computing",
	Volume = 6,
	Year = 1988,
	Pages = "349 -- 367",
	Annote = "Shows parallel QSort with random pivots doesn't speed up
		  beyond 10 processors."}

@article{rlt:par,
	Author = "K. Ramamohanarao and J. W. Lloyd and J. A. Thom",
	Title = "Partial-match retrival using hashing and
		 descriptors",
	Journal = "ACM Transactions on Database Systems",
	Year = 1984,
	Month = "December",
	Volume = "8",
	Pages = "552--576",
	Annote = "(not read) Apparently discusses using codewords
		  and descriptors (signatures) to speed partial
		  match query answering.  May be useful for
		  retrieving terms."}

@article{rs:rec,
	Author = "K. Ramamohanarao and R. Sacks-Davis",
	Title = "Recursive linear hashing",
	Journal = "ACM Transactions on Database Systems",
	Month = "September",
	Year = "1984",
	Volume = "9",
	Pages = "369--391",
	Annote = "(not read) a hashing scheme for dynamic files."}

@inproceedings{rs:asu,
	Author = "K. Ramamohanarao and R. Sacks-Davis",
	Title = "A superimposed codeword indexing scheme for very
		 large Prolog databases",
	Booktitle = "Proceedings of the Third International
		     Conference on Logic Programming",
	Year = 1986,
	Page = "569--576",
	Annote = "(not read)"}

@inproceedings{rs:ans,
	Author = "Kotagiri Ramamohanarao and John Shepherd",
	Title = "Answering Queries in Deductive Database Systems",
	Booktitle = "Proceedings of the Fourth International
		     Conference on Logic Programming",
	Year = 1987,
	Pages = "1014--1029",
	Annote = "An overview of some research on deductive database
	          systems.  Claims that we currently have the
		  techniques needed for efficient implementation
		  (``Magic Sets is extremely efficient.'')  They
		  discuss indexing for large relations - mostly
		  superimposed codeword approaches."}

@inproceedings{rs:apa,
	Author = "Louiqa Raschid and Snaley Y. W. Su",
	Title = "A Parallel Processing Strategy for Evaluating
	         Recursive Queries",
	Booktitle = "Proceedings of the Twelfth International
	             Conference on Very Large Databases",
	Year = 1986,
	Pages = "412--419",
	Annote = "Very preliminary stuff, essentially noting
	          that parts of Henschen-Naqvi can be
		  executed in a systolic fashion.  Probably
		  too fine-grained to be useful without
		  special purpose array processors."}

@article{rb:des,
	Author = "{A. L. Narasimha} Reddy and Prithviraj Banerjee",
	Title = "Design, Analysis, and Simulation of I/O Architectures
                 for Hypercube Multiprocessors",
	Journal = "IEEE Transactions on Parallel and Distributed Systems",
	Volume = 1,
	Number = 2,
	Month = "April",
	Year = 1990,
	Pages = "140--151",
	Annote = "Considers (1) how to embed I/O nodes within hypercubes
		  (not too interesting!) and (2) how to organize disk
		  systems (e.g., do you synchronize disks, how do you
		  stripe, etc.)  (2) is more interesting, but seems
		  fairly unrealistic --- how the heck do you synchronize
		  disks in a multicomputer?  The conclusions are that it
		  all depends on your expected access patterns."}

@inproceedings{rtw:the,
	Author = "{R. van} Renesse and A. S. Tanenbaum and A. Wilschut",
	Title = "The Design of a High-Performance file Server",
	Booktitle = "Proceedings of the Ninth International Conference
                     on Distributed Computer Systems",
	Publisher = "IEEE",
	Pages = "22-27",
	Year = 1989,
	Annote = "Describes the bullet file system.  It achieves it's
                  speed by 1. Storing files contiguously, both in RAM
                  and on disk, and 2. transferring files a file at a 
                  time (rather than block at a time), and 3. using
                  immutable files (updates create new versions.)
                  Performance studies showed it was 3-6 times faster
                  than NFS."} 

@article{rdhllmmp:pil,
	Author = "David D. Redell and Yoken K. Dalal and Thomas R. Horsley
                  and Hugh C. Lauer and William C. Lynch and Paul R. 
                  McJones and Hal G. Murray and Stephen C. Purcell",
	Title = "Pilot: An Operating System for a Personal Computer",
	Journal = "Communications of the ACM",
	Month = "February",
	Year = 1980,
	Number = 2,
	Volume = 23,
	Pages = "81--92",
	Annote = "Gives an interesting description of the mapped VM.
                  The only access to files is through mapping; the
                  only swapping store is provided by mapping VM to
                  files.  They claim the following benefits:
		  1)  Separating mapping and swapping decouples
                      buffer allocation from disk scheduling,
		  2)  Read/Write privileges can be inherited automatically
                      from the file to the mapped space.
                  3)  It may be easier to provide hints to avoid swapping,
                  4)  It's easy to simulate read/write interface on top of
                      mapping, but the reverse is hard."}

@inproceedings{rtygbbbc:mac,
	Title = "Machine-indpendent virtual memory management for
	         Paged Uniprocessor and Multiprocessor Architectures.",
	Booktitle = "Proceedings of the Second International Conference
	             on Architectural Support for Programming Languages
		     and Operating Systems.",
	Location = "Palo Alto, California",
	Year = 1987,
	Month = "October",
	Annote = "Revision of first Mach paper.  Good information 
	          on implementing VM systems."}

@article{rcs:the,
	Author = "Joel E. Richardson and Michael J. Carey and 
                  Daniel T. Schuh",
	Title = "The Design of the {E} Programming Language",
	Journal = "ACM Transactions on Programming Languages and Systems",
	Volume = 15,
	Number = 3,
	Month = "July",
	Year = 1993}

@techreport{rc:pro,
	Title = "Programming Constructs for Database
	           System Implementation in {EXODUS}",
	Author = "Joel E. Richardson and Michael J. Carey",
	Institution = "Computer Sciences Department, 
	               University of Wisconsin, Madison",
	Number = "680",
	Year = 1987,
	Month = "January",
	Annote = "E is the programming language for Exodus.
	          E is an extension of C++.  It differs
		  from ``database programming languages''
		  in that it is designed to be used by
		  the database implementor, rather than
		  the database user.  Because of this,
		  it explicitly deals with primary/secondary
		  storage issues.  One problem that must
		  be dealt with is that user types are
		  unknown at the time the system is built."}

@article{rc:per,
	Author = "Joel E. Richardson and Michael J. Carey",
	Title = "Persistence in the {E} Language: Issues
                 and Implementation",
	Journal = "Software Practice and Experience",
	Volume = 19,
	Month = "December",
	Year = 1989}

@inproceedings{rcb:sit,
	Author = "Arnon Rosenthal and Sharma Chakravarthy and 
		  Barbara Blaustein",
	Title = "Situation Monitoring for Active Databases",
	Booktitle = "Proceedings of the Fifteenth International Conference
		     on Very Large Databases",
	Month = "August",
	Year = "1989",
	Address = "Amsterdam, The Netherlands",
	Pages = "455--464",
	Annote = "(from the abstract) We present a basis for 
		  efficiently evaluating the situation (event and
		  condition)  portion of situation/action rules.
		  A common framework handles situations involving
		  both database changes and nondatabase situations.
		  We introduce $\delta$ Relations to represent net
		  changes to a stored or derived relation.  We define 
		  an operator that computes $\delta$ Relations for
		  derived relations.  Evaluation of expressions involving
		  changes is optimized by defining incremental forms
		  of relational operators and by providing a chain
		  rule that extends incremental computaiton to data
		  derived by arbitrary expressions."}


@article{rbfghlrsw:int,
	Author = "J. B. Rothnie and P. A. Bernstein and S. Fox and
	          N. Goodman and M. Hammer and T. A. Landers and
		  C. Reeve and D. W. Shipman and E. Wong",
	Title = "Introduction to a System for Distributed Databases
	         ({SDD-1})",
	Journal = "ACM Transactions on Database Systems",
	Month = "March",
	Volume = 5,
	Number = 1,
	Pages = "1--17",
	Year = 1980,
	Annote = "A distributed relational database for a long-haul
	          network.  Uses network communication as cost
		  measure for query planning, time-stamps for
		  concurrency control, two-phase commit for
		  recovery.  Separates query processing, reliable
		  writing, and concurrency control modules, and
		  claims this greatly simplifies the implementation."}

@techreport{rlw:one,
	Author = "Paul Rovner and Roy Levin and John Wick",
	Title = "On Extending {Modula-2} For Building Large, Integrated Systems",
	Institution = "DEC Systems Research Center",
	Number = 3,
	Year = 1985}

@inproceedings{r:ash,
	Author = "Lawrence A. Rowe",
	Title = "A Shared Object Hierarchy",
	Booktitle = "Proceedings of the ACM International Workshop on 
                     Object-Oriented Database Systems",
	Year = 1986,
	Month = "September",
	Address = "Pacific Grove, California",
	Pages = "160--170",
	Annote = "Describes how to implement an object store in Postgres.
             The main motivation for this is that an object store 
             requires almost a complete database anyway.  He claims
             a combination of prefetching, caching, triggers, and
             so forth will make this all real fast."}

@unpublished{r:ran,
	Author = "Shaibal Roy",
	Title = "Randomized Approximate Percentile Selection",
	Year = 1991,
	Institution = "Department of Computer Science, Stanford University",
	Note = "Unpublished Manuscript"}

@techreport{re:part,
	Author = "D. Ries and R. Epstein",
	Title = "Evaluation of distribution criteria for distributed
	         database systems",
	Institution = "UC-Berkeley",
	Number = "UCB/ERL Tech. Rep. M78/22",
	Month = "May",
	Year = 1978}

@article{rt:the,
	Author = "Dennis M. Ritchie and  Ken Thompson",
	Title = "The UNIX time-sharing system",
	Journal = "Communications of the ACM",
	Volume = 17,
	Number =  7,
	Month = "July",
	Year = 1974,
	Pages = "365--375"}



@inproceedings{r:top,
	Author = "Neil C. Rowe",
	Title = "Top-down statistical estimation on a database",
	Booktitle = "Proceedings of the ACM SIGMOD International
		     Conference on Management of Data",
	Address = "San Jose, California",
	Year = 1983,
	Month = "May",
	Pages = "135--144",
	Annote = "This approach works by first precomputing an
		  ``abstract'' of the database that is supposed
		  capture the database's statistical characteristics.
		  Then it uses an expert system to infer things
		  based upon what is found in the abstract.  Perhaps
		  most interesting is his discussion of what is
		  wrong with sampling: it cannot handle 
		  already-aggregated data, it is inefficient in paging,
		  it is inefficient in use of indexes (since it is
		  hard to guarantee that randomly sampling an index
		  results in a random sample of the data), and it is
		  no good at estimating extremums."}

@inproceedings{rkc:ben,
	Author = "W. Rubenstein and M. Kubicar and R. Cattell",
	Title = "Benchmarking Simple Database Operations",
	Booktitle = "Proceedings of the ACM SIGMOD Conference",
	Address = "San Francisco, California",
	Month = "May",
	Year = 1987}

@article{c:est2,
        Author = "Stavros Christodoulakis",
        Title = "Estimating Record Selectivities",
        Journal = "Information Systems",
        Number = 2,
        Volume = 8,
        Pages = "69--79",
	Year = 1983,
        Annote = "Uses paramtric approach based on a family
                  of probability density functions."}

@inproceedings{stgsuv:fas,
	Author = "Betty Salzberg and Alex Tsukerman and Jim Gray and
		Susan Uern and Bonnie Vaughan",
	Title = "{FastSort}: A Distributed Single-Input Single-Output 
		External Sort",
	Booktitle = "Proceedings of the ACM-SIGMOD International Conference
                     on Management of Data",
	Year = 1990,
	Month = "May",
	Address = "Atlantic City, New Jersey",
	Pages = "94--101",
	Annote = "Describes a single-input single-output sort.  First,
		the file to be sorted is partitioned round-robin to 
		a number of processing sites.  Each site sorts the
		incoming data using tree-selection sort.  The paper
		has a good discussion of that algorithm, which has
		the two advantages that  (1) it forms runs that are
		twice as long as memory size, and (2) sorting begins
		as soon as data arrives (no need to fill memory before 
		starting.)  After each site has completed it's local
		sort, the runs are merged at a single site, and returned
		to the target destination.  Interestingly, claims that
		the algorithm is linear time.  What's really going on is
		that enough sites are present to make the internal sort
		and merge phases fast enough that the initial scan is
		the bottleneck.  It's really only linear for a 
		range of data sizes.  Also, points out that even for
		a 1 Tbyte file, only 100Mbyte of memory is needed to
		sort a file in two passes."}

@inproceedings{sgkwl:des,
	Author = "R. Sandberg and D. Goldberg and S. Kleiman 
                  and D.Walsh and B.Lyon",
	Title = "Design and Implementation of the Sun Network Filesystem",
	Booktitle = "USENIX Summer Conference Proceedings", 
	Year = 1985}


@book{s:alg,
	Author = "Robert Sedgewick",
	Title = "Algorithms",
	Publisher = "Addison-Wesley",
	Year = 1988,
	Address = "Reading, Massachusetts",
	Note = "Second Edition."}

@inproceedings{sc:ape,
	Author = "Eugene J. Shekita and Michael J. Carey",
	Title = "A Performance Evaluation of Pointer-Based Joins",
	Booktitle = "Proceedings of the ACM-SIGMOD International Conference
                     on Management of Data",
	Year = 1990,
	Month = "May",
	Address = "Atlantic City, New Jersey",
	Pages = "300--312",
	Annote = "Finds that pointer-based joins work well as long as
                  you are careful (and don't use nested loops!)"}
}

@inproceedings{scd:per,
	Author = "D. Schuh and M. Carey and D. DeWitt",
	Title = "Persistence in {E} Revisited --- Implementation Experiences",
	Booktitle = "Proceedings of the Fourth International Workshop on 
                     Persistent Object Systems",
	Publisher = "Morgan Kaufmann",
	Year = 1991}

		


@inproceedings{sr:pre,
	Author = "Jaideep Srivastava and Doron Rotem",
	Title = "Precision-Time Tradeoffs: A Paradigm for Processing
                 Statistical Queries on Databases",
	Booktitle = "Proceedings of the Fourth International Working Conference on
		     Statistical And Scientific Database Management",
	Month = "June",
	Year = "1988",
	Address = "Rome, Italy",
	Publisher = "Springer-Verlag",
	Pages = "226--245",
	Annote = "The idea is to maintain a view of some statistic of the base
                  data, for fast querying.  The main question addressed is how
                  to maintain the accuracy of the view in the presence of updates.
                  The approach is to model updates as a Poisson process, then based
                  on this model decide when the view has to be refreshed."}

@article{sg:sys,
	Author = "Kenneth Salem and Hector Garcia-Molina",
	Title = "{System M}: A Transaction Processing Testbed for
		Memory Resident Data",
	Journal = "IEEE Transactions on Knowledge and Data Engineering",
	Month = "March",
	Year = 1990,
	Volume = 2,
	Number = 1,
	Pages = "161 --172",
	Annote = "System M is designed to be a testbed for various
		strategies for logging and checkpointing main memory
		databases.  The main conclusion seems to be that
		while fuzzy checkpointing is best, you can come pretty
		close with consistent checkpointing schemes.

		Some specific comments in relation to TPK:

		1) Maintains two disk copies, claiming that this
		is necessary for consistent checkpoints (not true
		if have two memory copies).

		2) Claims two backups are more reliable.  This isn't
		true if you're constantly checkpointing, which is the
		case in their system.

		3) Uses two raw partitions (on separate disks), one
		for logging, the other for checkpointing.

		4) Experiments get about 70 TPS, but with 4 sec response
		time!  Also, claims 11/785 is 1 MIP."}


@techreport{sg:cra,
	Author = "Kenneth Salem and Hector Garcia-Molina",
	Title = "Crash Recovery Mechanisms for Main Storage
	         Database Systems",
        Year = 1986,
	Number = "CS-TR-034-86",
	Institution = "Department of Computer Science, Princeton
	               University"}

@techreport{sg:cra87,
	Author = "Kenneth Salem and Hector Garcia-Molina",
	Title = "Crash Recovery for Memory-Resident Databases",
	Year = 1987,
	Number = "CS-TR-119-87",
	Institution = "Department of Computer Science,
	               Princeton University"}

@techreport{sg:che,
	Author = "Kenneth Salem and Hector Garcia-Molina",
	Title = "Checkpointing Memory-Resident Databases",
	Year = 1987,
	Number = "CS-TR-126-87",
	Institution = "Department of Computer Science, 
		       Princeton University"}

@inproceedings{sg:dis,
	Author = "Kenneth Salem and Hector Garcia-Molina",
	Title = "Disk Striping",
	Booktitle = "Proceedings of the International
	             Conference on Data Engineering",
	Pages = "336--342",
	Month = "February",
	Year = 1986}

@inproceedings{sd:ape,
	Author = "Donovan A. Schneider and David J. DeWitt",
	Title = "A Performance Evaluation of Four Parallel Join
		 Algorithms in a Shared-Nothing Multiprocessor
		 Environment",
	Booktitle = "Proceedings of the ACM-SIGMOD International
		     Conference on Management of Data",
	Year = 1989,
	Month = "June",
	Address = "Portland, Oregon",
	Pages = "110--121",
	Annote = "Analyzes and compares four parallel join
		  algorithms: simple hash, hybrid hash,
		  grace, and sort-merge.  The results are
		  that hybrid hash wins except when
		  join attribute of inner relation is
		  nonuniformly distributed and memory
		  is limited.  Performance figures
		  are gained by implementation on GAMMA 
		  (laundromat version.)"}

@article{snos:rap,
	Author = "S. A. Schuster and H. B. Nguyen and E. A. 
		  Ozkarahan and K. C. Smith",
	Title = "{RAP}.2 --- An Associative Processor for Databases
	         and its Applications",
	Journal = "IEEE Transactions on Computers",
	Volume = "c-28",
	Number = 6,
	Month = "June",
	year = 1979}

@inproceedings{scflmmp:ext,
	Author = "P. Schwarz and W. Chang and J. C. Freytag and
		  G. Lohman and J. McPherson and C. Mohan and
		  H. Pirahesh",
	Title = "Extensibility in the {Starburst} Database System",
	Booktitle = "Proceedings of the International Workshop
		     on Object-oriented Database Systems",
	Address = "Asilomar, California",
	Month = "September",
	Year = 1986}

@unpublished{slm:des,
	Author = "Michael L. Scott and Thomas J. LeBlanc and Brian D. Marsh",
	Title = "Design Rationale for Psyche, a general-purpose multiprocessor
	         operating system",
	Institution = "University of Rochester",
	Note = "In Computer Science and Engineering Research
			Review, 1988-89"}

@article{s:the,
	Author = "Charles L. Seitz",
	Title = "The Cosmic Cube",
	Journal = "Communications of the ACM",
	Volume = 28,
	Number = 1,
	Pages = "22--33",
	Month = "January",
	Year = 1985}

@inproceedings{sn:par,
	Author = "S. Seshadri and Jeffrey F. Naughton",
	Title = "Sampling Issues in Parallel Database Systems",
	Booktitle = "Proceedings of the EDBT Conference",
	Month = "March",
	Year = 1992,
	Address = "Vienna, Austria"}

@techreport{s:per,
	Author = "Peter Szeredi",
	Title = "Performance Analysis of the Aurora Or-Parallel Prolog System",
	Year = 1989,
	Month = "September",
	Number = "TR-89-14",
	Institution = "University of Bristol Computer Science Department",
	Annote = "Gives a detailed summary of data gathered by instrumenting
		  Aurora.  Concentrates on idle time and parallel overheads."}
		  

@inproceedings{sk:tem,
	Author = "Arie Shoshani and Kyoji Kawagoe",
	Title = "Temporal Data Management",
	Booktitle = "Proceedings of the Twelfth International Conference
		     on Very Large Databases",
	Year = 1986,
	Address = "Kyoto, Japan",
	Pages = "79--88",
	Annote = "The authors consider storing data in what Snodgrass
		  would probably consider a roll-back database.  The
		  data stored can be thought of as triples, 
		  <UID, Value, Time>.  Each UID defines an object,
		  and the set of triples for a given UID forms a
		  Time Sequence (TS.)  The authors give a classification
		  for TSs that arise: regular/irregular, 
		  discrete/continuous/step-wise constant,
		  static/dynamic, time unit/start time/end time.
		  They also consider some operations on TSs:
		  restriction, composition, etc.  This is followed
		  by some preliminary ideas on storage structures,
		  designed to give indexed lookup on UID's and
		  time, but not data.  Essentially, they say some
		  dynamic, grid-file that grows only at the edges
		  needs to be invented."}

@article{s:log,
	Author = "D. L. Slotnik",
	Title = "Logic per Track Devices",
	Journal = "ACM Transactions on Database Systems",
	Volume = 1,
	Number = 3,
	Month = "September",
	Year = 1976}

@article{snodgrass:the,
	Author = "Richard Snodgrass",
	Title = "The Temporal Query Language {TQuel}",
	Journal = "ACM Transactions on Database Systems",
	Volume = 12,
	Number = 2,
	Month = "June",
	Year = 1987,
	Pages = "247--298",
	Annote = "TQuel is an extension of QUEL for Temporal
		  Databases.  The paper gives the syntax and
		  semantics for TQuel.  The principle extensions
		  are to the where clauses --- adding when, 
		  and valid from ... to, as of .. through.  Each
		  of these extensions uses temporal predicates
		  to specify time intervals and operations on them.
		  The temporal predicates are similar to path
		  expressions.  The paper terminates with a list
		  of 17 desirable properties of temporal query
		  languages, and how many such languages proposed
		  to date stack up."}

@inproceedings{sa:ata,
	Author = "Richard Snodgrass and Ilsoo Ahn",
	Title = "A Taxonomy of Time in Databases",
	Booktitle = "Proceedings of the SIGMOD International Conference
		     on Management of Data",
	Month = "May",
	Year = 1985,
	Pages = "236--245",
	Annote = "Defines three types of time in a database system:
		
		  1. Transaction Time, the time data was entered into
		     the database.
		  2. Valid Time, the time at which the data was valid
 		     in reality.
		  3. User Defined time, everything else.
		 
		  Only types one and two are interpreted by the system.
		  Also defines four types of DBMS: static, static rollback,
		  historical, and temporal.  Static models without respect
                  to time.  Static rollback lets you ask questions about
                  previous versions of the data, but these versions are
                  read-only.  Historical stores one instance, but represents
                  lots of instants of time in that instance.  All are 
                  updatable.  Temporal combines both static rollback and
                  temporal."}

@article{as:tem,
	Author = "Richard Snodgrass and Ilsoo Ahn",
	Title = "Temporal Databases",
	Journal = "IEEE Computer",
	Volume = 19,
	Number = 9,
	Month = "September",
	Year = 1986,
	Pages = "35--42",
	Annote = "Essentially a readers-digest combination of
		  their SIGMOD paper and Snograss's TODS
		  paper."}



@inproceedings{seddbdpt:hig,
	Author = "Alfred Z. Spector and Jeffrey L. Eppinger and
		  Dean S. Daniels and Richard Draves and
	          Joshua J. Bloch and Dan Duchamp and
		  Randy F. Pausch and Dean Thompson",
	Title = "High Performance Distributed Transaction Processing
		  in a General Purpose Computing Environment",
	Booktitle = "Proceedings of the Second International Workshop
		     on High Performance Transaction Systems",
	Month = "September",
	Year = 1987}

@techreport{stpedddb:cam,
	Author = "Alfred Z. Spector and Dean Thompson and Randy F. Pausch
		and Jeffrey L. Eppinger and Dan Duchamp and Richard Draves
		and Dean S. Daniels and Joshua J. Bloch",
	Title = "Camelot: A Distributed Transaction Facility for {M}ach
                 and the Internet --- {A}n Interim Report",
	Institution = "Carnegie Mellon University, Department of Computer 
                       Science",
	Number = "CMU-CS-87-129",
	Month = "June",
	Year = 1987,
	Annote = "overview of Camelot."}

@article{s:sta,
	Author = "James W. Stamos",
	Title = "Static Grouping of Small Objects to Enhance Performance
                 of a Paged Virtual Memory",
	Journal = "ACM Transactions on Computer Systems",
	Volume = 2,
	Number = 2,
	Month = "May",
	Year = 1984,
	Pages = "155-180",
	Annote = "Concludes that the strategies examined are all about
                  the same, and are midway between random and optimal.
                  Main candidates were DFS or BFS, each with either
                  compiler hints, refrence counts, or dynamic statistics
                  used to prioritize references within objects."}

@techreport{sy:asy,
	Author = "James W. Stamos and Honesty C. Young",
	Title = "A Symmetric Fragment and Replicate Algorithm for
                 Distributed Joins",
	Institution = "IBM Research Division",
	Address = "Almaden Research Center, San Jose, California",
	Number = "RJ 7118 (67667)",
	Month = "December",
	Year = 1989,
	Annote = "Proposes an improvement of fragment-replicate that
                  partially fragments, partially replicates both
                  relations, thus cutting down on communication costs."}

@article{s:ase,
	Author = "John A. Stankovic",
	Title = "A Serious Problem for Next-Generation Systems",
	Journal = "IEEE Computer",
	Volume = 21,
	Number = 10,
	Month = "October",
	Year = 1988,
	Pages = "10--18",
	Annote = "Makes the case for a scientific study of
                  real time computing, that is, computations
                  such that the time at which an answer is
		  produced is as important as the logical
		  correctness of the answer.  Gives the
		  impression that right now things are too
		  ad-hoc.  Says we need 1) languages and
		  semantics for expressing and reasoning
                  about realtime 2) scheduling theory designed
                  to minimize missed deadlines (rather than 
                  total waiting time or 1/throughput) 3)
		  OS work and a bunch of other stuff as well."}

@inproceedings{sr:the,
        Author = "John A. Stankovic and Krithi Ramamritham",
        Title = "The Design of the Spring Kernel",
        Booktitle = "Proceedings of the IEEE Real-Time Systems Symposium",
        Month = "December",
        Year = 1988,
        Address = "San Jose, California",
        Pages = "146--157",
        Annote = "Designed to support complex hard real-time systems
                  in which the system has perfect knowledge about the
                  timing and resource requirements of all the tasks
                  to be run.  Contains general real-time kernel
                  characteristics such as fast context switch, small
                  size, quick interrupt response, etc.  A novel feature
                  is that the scheduler guarantees mutual exclusion,
                  hence no need for locks or semaphores, which can
                  cause unpredictable waits and many context switches.
                  Uses a greedy scheduling algorithm.  Uses multiple
                  processors, some for system tasks, some for application,
                  some for interrupts."}


@article{s:ope,
	Author = "Michael Stonebraker",
	Title = "Operating System Support for Database Management",
	Journal = "Communications of the ACM",
	Year = 1981,
	Volume = 24,
	Number = 7,
	Pages = "412--418",
	Annote = "The general idea is that OS don't provide the 
		  right services so that DB implementors wind up
		  duplicating things.  For example, the db has
		  information about future access patterns that
		  the buffer manager should be able to use.  Also,
		  too much context switching is required to do
		  reads.  Virtual memory mapping of relations
		  may help, but also have a lot of overhead
		  (space)."}

@article{s:vir,
	Author = "Michael Stonebraker",
	Title = "Virtual Memory Transaction Management",
	Booktitle = "Operating Systems Review",
	Volume = 18,
	Number = 2,
	Month = "April",
	Year = "1984",
	Pages = "8--16",
	Note = "Claims that there are four main problems:
		1) must use page-level locking.
		2) only pages that are locked can be addressible.
		3) the OS has no knowledge about semantics of 
		   pages, i.e., indexes vs. data, etc.
		4) can't use OS page replacement policy in all
		   cases.

		It seems to me that there are a bunch of
		assumptions here, for example, he doesn't
		consider the possibililty of implementing
		a DB style transaction monitor in VM.  But
		maybe that's what's done now?"}

@inproceedings{srg:app,
	Author = "Michael Stonebraker and Brad Rubenstein and 
		  Antonin Guttman",
	Title = "Application of Abstract Data Types and Abstract
	          Indices to {CAD} Data",
	Booktitle = "Proceedings of the IEEE Annual Meeting Database
		     Week",
	Year = 1983,
	Pages = "107--115",
	Annote = "Describes the addition of ADT's to Ingres.  Examples
		  are all CAD, e.g., Box type for VLSI.  Also initial
		  approach to ADT secondary indices."}

@inproceedings{sjgp:onr,
	Author = "Michael Stonebraker and Anant Jhingran and Jeffrey Goh
                  and Spyros Potamianos",
	Title = "On Rules, Procedures, Caching, and Views in Data Base
                 Systems",
	Booktitle = "Proceedigns of the ACM-SIGMOD International Conference
                     on Management of Data",
	Year = 1990,
	Month = "May",
	Address = "Atlantic City, New Jersey",
	Pages = "281--290",
	Annote = "Describes the second rule system for POSTGRES and how
                  it can be used to implement everything.  Two implementation
                  techniques are described: tuple locks, and query rewrite.
                  Also, an algorithm for caching materialized views and
                  managing that cache is given."}

@article{swrmma:per,
	Author = "Michael Stonebraker and John Woodfill and
	          Jeff Ranstrom and Marguerite Murhpy and
		  Eric Allman",
	Title = "Performance Enhancements to a Relational
	         Database System",
	Journal = "ACM Transactions on Database Systems",
	Year = 1983,
	Volume = 8,
	Number = 2,
	Month = "June",
	Pages = "167--185",
	Annote = "Discusses four performance enhancements
		  for DBMS.  1) Compiling queries (e.g.,
		  noticing when an int will be compared
		  to a real field, and doing the conversion
		  once instead of interpretively once per
		  tuple.)  2) Special Microcode Procedures.
		  3) Special file system (extents, no system
		  buffering before system gets it.)  4) 
		  special OS (eliminates context switching
		  overhead, and cheapens message passing.)
		  
		  The conclusion is that 1) and 3) are a
		  big win, 2) and 4) are not."}

@article{stone:sn,
	Author = "M. Stonebraker",	
	Title = "The case for shared nothing.",
	Journal = "Database Engineering",
	Volume = 9,
	Number = 1,
	Year = 1986}



@article{sde:pro,
	Author = "Michael Stonebraker and Debora DuBourdieux
	          and William Edwards",
	Title = "Problems in Supporting Data Base Transactions
		 in an Operating System Transaction Manager",
	Journal = "Operating Systems Review",
	Year = 1984,
	Pages = "6--14",
	Annote = "First problem: Ingres writes block at a time.
		  If OS writes in smaller units, this could be
		  suboptimal; but if Ingres is modified to write
		  in smaller units, need two updates per write,
		  one for changed line, one for entry to tell where
		  changed line is in block.

		  Second problem: Ingres writes db catalogs
		  upon update (updating number of tuples, adding
		  relations, etc.)  This can result in locking
		  many entire relations on a simple update to one.
		  The authors advocate non-two phase locking for
		  system catalog, but this has correctness
		  implications.  They assert that this can be
		  remedies by providing ``reverse'' operations
		  (instead of simply copying old page image
		  back, reverse action of aborting transactions,
		  e.g., decrement a count if operation incremented.
		  It seems to me that this only works for
		  a certain class of problems --- those for
		  which the update actions are commutative.)
		  
		  In summary, the authors advocate providing user 
		  events in system logs, not just pre- and
	          post-imaging."}

@inproceedings{s:asc,
        Author = "Alexander D. Stoyenko",
        Title = "A Schedulability Analyzer for Real-Time Euclid",
        Booktitle = "Proceedings of the IEEE Real-Time Systems Symposium",
        Month = "December",
        Year = 1987,
        Address = "San Jose, California",
        Pages = "218--227",
        Annote = "Real-Time Euclid is a subset of Euclid designed to
                  be completely predictable --- all loops constant
                  counts, no recursion, no runtime storage allocation,
                  and so forth.  By exhaustively analysing a static
                  set of processes (down to the assembly instruction
                  level, and including all possible overlaps) he
                  is able to decide if something is really schedulable.
                  This work is concerned with hard realtime.  Some
                  of the more novel features include the analysis
                  of queueing on semaphores and monitors, and the
                  division into interruptible and not interruptable
                  code segments."}

@article{sy:opt,
	Author = "Robert E. Strom and Shaula Yemini",
	Title = "Optimistic Recovery in Distributed Systems",
	Journal = "ACM Transactions on Computer Systems",
	Volume = 3,
	Number = 3,
	Month = "August",
	Year = 1985,
	Pages = "204--226",
	Annote = "Proposes a way to support application-independent transparent
		recovery from processor failures in distriubted systems.  
		Communication, computation, and checkkpointing proceed
		asynchronously.  Uses causal dependency tracking during
		execution and process rollback and message replay
		during recovery.  Overheads during normal execution
		are: 1) appending a session sequence number to each message
		and checking it on arrival; 2) maintaining a dependency
		vector in each recovery unit, copying it to the head
		of every message sent, and updating the dependency vector
		on each message received (one word per processor!!),
		3) periocically checkpointing the full state of each
		recovery unit and incrementally logging input messages,
		4) periodically transmitting and updating the log vector,
		and 5) buffering messages until they are committable."}

@inproceedings{syb:are,
	Author = "Robert E. Strom and Shaula A. Yemini and David F. Bacon",
	Title = "A Recoverable Object Store",
	Booktitle = "Proceedings of the Twenty-First Annual Hawaii
	             International Conference on System Sciences",
	Year = 1988,
	Month = "January",
	Address = "Honolulu, Hawaii",
	Pages = "215 -- 221",
	Annote = "Describes a layered approach to building a self-recovering
		OS.  The main idea is to use the authors' optimistic recovery
		scheme to deal with messages, and to periodically take
		checkpoints, being clever about what's already on disk and
  		using shadow pages to avoid massive checkpointing overheads.
		It seems like the message logging overhead could be significant
		if this system were implemented on a multicomputer."}

@techreport{s:dat,
	Author = "Bjarne Stroustrup",
	Title = "Data Abstraction in {C}",
	Institution = "Bell Laboratories",
	Year = 1984,
	Number = "Computing Science Technical Report Number 109",
	Annote = "Three papers about C++"}.

@article{su:the,
	Author = "S. Y. W. Su and et al.",
	Title = "The Architectural Features and Implementation 
	         Techniques of the Multicell {CASSM}",
	Journal = "IEEE Transactions on Computers",
	Volume = "c-28",
	Number = 6,
	Year = 1979}

@article{s:pvm,
	Author = "V. Sunderam",
	Title = "{PVM}: A Framework for Parallel Distributed Computing",
	Journal = "Concurrency: Practice and Experience",
	Volume = 2,
	Number = 4,
	Month = "December",
	Year = 1990}

@techreport{trygtbs:aun,
	Author = "Avadis {Tevanian, Jr.} and Richard F. Rashid and
      		Michael W. Young and David B. Golub and 
		Mary R. Thompson and William Bolosky and
		Richard Sanzi",
	Title = "A Unix Interface for Shared Memory and Memory Mapped
		 Files Under Mach",
	Month =  "July",
	Year = 1987,
	Institution = "Carnegie Mellon University",
	Annote = "
		First makes a few claims about UNIX mmap: 
		1. UNIX files must always be persistent, which
		   imposes too much overhead for IPC.
		2. A mapped file facility must handle network
		   file systems such as NFS, complicating the kernel.
		3. Sharing semantics are under the control of the
		   kernel, not the application.
		Then identifies four uses of shared memory:
		1. fine granularity multiprocessing.
		2. ultra-fast IPC.
		3. database management support.
		4. reduced overhead file management.
		The claim is MACH handles 1. through threads and
		restricted inheritance of virtual address spaces, 
		2. through copy-on-write shared physical memory,
		3. and 4. through external pagers.  Roughly,
		each memory has its own server, which manages
		the object in response to calls from the kernel.
		Files can be mapped by creating such a server for
		the file."}


@inproceedings{ts:fir,
	Author = "C. P. Thacker and L. C. Stewart",
	Title = "Firefly: a multiprocessor workstation",
	Booktitle = "Proceedings of the Second International
		     Conference on Architectural Support for
		     Programming Languages and Operating
		     Systems",
	Pages = "164--172",
	Month = "October",
	Year = 1987,
	Annote = "Overview of the Firefly."}

@inproceedings{t:abe,
	Author = "The {Tandem Performance Group}",
	Title = "A Benchmark of {NonStop} {SQL} on the Debit-Credit
		 Transaction",
	Booktitle = "Proceedings of the SIGMOD International
		     Conference on Management of Data",
	Year = 1988,
	Month = "May",
	Address = "Chicago, Illinois",
	Pages = "337--341",
	Annote = "They get about 200 TPS at 2 second response with
	          32 3 MIP processors.  Appears to be very carefully
		  done - the benchmark was audited by Date/Codd 
		  Consulting.  They mention scaling the database,
		  something I missed from the anon paper.  1 TPS
		  means 100000 accounts, 100 Tellers, 10 Branches,
		  2590000 history records."}
		  
@inproceedings{trn:asu,
	Author = "James A. Thom and Kotagiri Ramamohanarao and Lee Naish",
	Title = "A Superjoin Algorithm for Deductive Databases",
	Booktitle = "Proceedings of the Twelfth International Conference
		     on Very Large Databases",
	Month = "August",
	Year = 1986,
	Annote = "Discusses a technique of using multi-key hashing and
	          buffering to reduce number of disk accesses in joins.
		  Should read it more carefully sometime.  The basic idea
		  is to use multi-keys to limit the number of pages of
		  relation 2 that could possibly join with tuples of
		  relation 1."}

@article{t:vir,
	Author = "Irving L. Traiger",
	Title = "Virtual Memory Management for Database Systems",
	Journal = "Operating Systems Review",
	Month = "October",
	Year = 1982,
	Volume = 16,
	Number = 4,
	Pages = "26--48",
	Annote = "Describes how System R handles things.  The DBMS
		  maps things into and out of its virtual memory;
		  the OS doesn't know anything about this.  This
		  Causes some inefficiencies --- bad buffering, 
		  storing pages on disk twice or more, etc.
		  He also describes this with respect to two
		  reliability techniques: shadow paging and
		  write ahead logs.  His conclusion is that
		  using the virtual memory directly is elegant
		  and efficient, but you have to be careful
		  for reliability, because you have much
		  less control over when things are forced
		  out to disk, etc."}

@unpublished{u:nai,
	Author = "Jeffrey D. Ullman",
	Title = "{NAIL!} {ICODE} summary: {C/SQL} version",
	Year = 1986,
	Note = "Unpublished memorandum, Stanford University"}



@article{vg:joi,
	Author = "Patric Valduriez and Georges Gardarin",
	Title = "Join and Semijoin Algorithms for a Multiprocessor
	         Database Machine",
	Journal = "ACM Transactions on Database Systems",
	Year = 1984,
	Volume = 9,
	Number = 1,
	Month = "March",
	Pages = "133--161",
	Annote = "Work done as part of the SABRE database machine
		  project at INRIA.  Conclusions were that
		  nested loops is good with lots of processors,
		  sort-merge-join good if relation size is
		  relatively large.  Note: their algorithm
		  uses multiple processors for the sort, but
		  only one does the merge.  Hashing join is
		  good if number of matching tuples is small.
		  In that algorithm, a boolean hash table on 
		  the join attribute is built.  If relations from
		  both tuples hash to the same bucket, they are
		  candidates to participate in the join.
		  
		  Also found that, in general, first doing
		  a hashed semi-join then doing a join is
		  better than doing just the join."}

@inproceedings{vkc:imp,
	Author = "Patrick Valduriez and Setrag Khoshafian and George
		  Copeland",
	Title = "Implementation techniques of Complex Objects",
	Booktitle = "Proceedings of the Twelfth VLDB",
	Year = 1986,
	Pages = "101--110",
	Address = "Kyoto, Japan",
	Month = "August",
	Annote = "Discusses techniques for storing complex objects.
		  Seems to repeat a lot of old Network and Hierarchy
		  results.  Also, discusses storing objects by
		  ``decomposition,'' then using join indices
		  to facilitate retrieval."}

@article{v:joi,
	Author = "Patrick Valduriez",
	Title = "Join Indices",
	Journal = "ACM Transactions on Database Systems",
	Volume = 12,
	Number = 2,
	Month = "June",
	Year = 1987,
	Pages = "218 -- 246",
	Annote = "A join index is a relation that contains tuples
                 of pairs of tid's (r_tid, s_tid) for matching tuples
                 r and s in R and S.  The basic join index algorithm
                 is to scan the join index, reading the referenced tuples
                 of R and S as you go.  His experiments showed that 
                 this (when augmented with sorting surrogated and 
                 batching use of memory pages) beat hybrid hash."}

@unpublished{vln:glo,
	Author = "Shivakumar Venkataraman and Miron Livny and Jeffrey F. 
                  Naughton",
	Title = "Global memory management for parallel {OODBMS}",
	Month = "December",
	Year = "1993",
	Note = "Submitted for publication."}


@article{vc:asy,
	Author = "Paul Vongsathorn and Scott D. Carson",
	Title = "A System for Adaptive Disk Rearrangement",
	Journal =  "Software---Practice and Experience",
	Volume = 20,
	Number = 3,
	Pages = "225--242",
	Month = "March",
	Year = 1990,
	Annote = "First claims that disk cylinder accesses can be
                  modeled by a Markov chain.  Minimizing average seek
                  distance by using this model is impractical, so they
                  assume probabilistic independence, which gives the
                  organ pipe arrangement as the best.  Their system
                  sits in the device driver, and 1. Measures access
                  patterns; 2. computes better cylinder arrangements;
                  3.  rearranges the cylinders.  Experience with the
                  UNIX file system showed a big improvement."} 

@inproceedings{wpekt:the,
	Author = "B. Walker and G. Popek and R. English and C. Kline
		  and G. Thiel",
	Title = "The {LOCUS} distributed operating system",
	Booktitle = "Proceedings of the 9th ACM Symposium on Operating
                     System Principles",
	Month = "October",
	Year = "1983",
	Pages = "49--70"}

@inproceedings{wdj:ata,
    author="Christopher B. Walton and Alfred G. Dale and Roy M. Jenevein",
    title="A taxonomy and performance model of data skew effects in 
	   parallel joins",
    booktitle="Proceedings of the Seventeenth International Conference on
               Very Large Data Bases",
    year="1991",
    month="September",
    address="Barcelona, Spain"}

@inproceedings{wr:cac,
Author = "Y. Wang and L. Rowe", 
Title = "Cache Consistency and Concurrency Control 
         in a Client/Server DBMS Architecture", 
Booktitle = "Proceedings of the ACM SIGMOD Conference",
Address = "Denver, CO", 
Month = "June", 
Year = 1991}

	

@inproceedings{wplp:tra,
	Author = "Matthew J. Weinstein and Thomas W. Page and 
		  Brian K. Livezey and Gerald J. Popek",
	Title = "Transactions and Synchronization in a Distributed 
		 Operating System",
	Booktitle = "Proceedings of the Tenth ACM Symposium on Operating
		     System Principles",
	Year = 1986}

@inproceedings{wf:set,
	Author = "Jennifer Widom and Sheldom J. Finkelstein",
	Title = "Set-Oriented Production Rules in Relational Database
                 Systems",
	Booktitle = "Proceedings of the SIGMOD International Conference
                     on Management of Data",
	Year = 1990,
	Month = "May",
	Address = "Atlantic City, New Jersey",
	Pages = "259--270",
	Annote = "Describes a rule system cast as an extension to SQL.
                  Set-oriented means the condition parts of rules deal
                  with sets rather than individual tuples.

		  The main ideas are: 1) characterize database operations
                  as a series of blocks.  2) Each block produces three
                  sets of affected tuples: Inserted, Deleted, and Updated.  
                  3)  Blocks map states to states and in addition produce
                  the triples of affected tuples.  Trigger conditions are
                  stated in terms of these states and transitions."}

@inproceedings{wd:ape,
	Author = "S. White and D. DeWitt",
	Title = "A Performance Study of Alternative Object Faulting and
                 Pointer Swizzling Strategies",
	Booktitle = "Proceedings of the VLDB Conference",
	Address = "Vancouver, British Columbia",
	Month = "August",
	Year = 1992}

@inproceedings{wm:dem,
	Author = "Paul R. Wilson and Thomas G. Moher",	
	Title = "Demonic Memory for Process Histories",	
	Booktitle = "Proceedings of the ACM SIGPLAN Conference
		     on Programming Language Design and Implementation",
	Address = "Portland, Oregon",
	Month = "June",
	Year = 1989,
	Pages = "330--343",
	Annote = "Demonic memory presents the illusion that every state
		  of every page of a process has been saved.  Underneath,
		  time is divided into chapters, and points in between
		  chapters are reconstructed by re-executing that 
		  portion of the process.  Lots of efficiency tricks,
		  such as less and less frequent checkpoints further
		  back in time, tree-structured access to stored data,
	  	  a generational, compacting garbage collector."}

@techreport{w:poi,
	Author = "Paul R. Wilson",
	Title = "Pointer Swizzling at Page Fault Time: Efficiently
		Supporting Huge Address Spaces on Standard 
		Hardware",
	Institution = "EECS Department, University of Illinois at Chicago",
	Year = 1990,	
	Month = "December",
	Number = "UIC-EECS-90-6",
	Annote = "Basic idea: when a page p is read into memory,
	 	(1) fault in all pages p' into which pointers on p
                    point, and
                (2) update the pointers on p, and
	 	(3) write protect all pages p' to detect future
                    accesses.  
         Requires that (1) you know data format, and that a persistent
         page requires no more space than a transient page."}

@inproceedings{wc:dat,
	Author = "M. Winslett and S. Chu",
	Title = "Database Management Systems for {ECAD} Applications:
                 Architecture and Performance",
	Booktitle = "Proceedings of the NSF Conference on Design
                     and Manufacturing Systems",
	Address = "Atlanta, Georgia",
	Month = "January",
	Year = 1992}


@article{w:deb,
	Author = "Larry D. Wittie",
	Title = "Debugging Distributed {C} Programs by
                 Real-Time Replay",
	Journal = "SIGPLAN Notices",
	Volume = 24,
	Number = 1,
	Month = "January",
	Year = 1989,
	Pages = "57--67",
	Annote = "For checkpointing, requires that all IPC with system-provided
                  mechanisms (no shared memory.)  Threads synchronously
                  stop every 30 seconds, checkpoint, then restart
                  after another 0.5 seconds."}

@inproceedings{wdy:ane,
	Author = "Joel L. Wolf and Daniel M. Dias and Philip S. Yu",
	Title = "An Effective Algorithm for Parallelizing Sort Merge Joins
	         in the Presence of Data Skew",
	Booktitle = "Proceedings of the Second International Symposium on
		     Databases in Parallel and Distributed Systems",
	Month = "July",
	Year = 1990,
	Address = "Dublin, Ireland",
	Pages = "103--115",
	Annote = "The main idea is to first completely scan the two
		  relations to be joined, then figure out how to partition
		  the relations so that each processor has about the same
		  amount of work to do in the join.  In addition to the
		  extra scan of the relations, the 'scheduling' phase seems
		  to involve a lot of extra work."}

@unpublished{wdyt:ane,
	Author = "Joel L. Wolf and Daniel M. Dias and Philip S. Yu and John J. Turek",
	Title = "An Effective Algorithm for Parallelizing Hash Joins
	         in the Presence of Data Skew",
	Year = 1990,
	Note = "IBM T. J. Watson Research Center Tech Report RC 15510",
	Annote = "Almost identical to~\cite{wdy:ane}, except that now the
		  number of tuples sent to each processor is estimated by
		  a set of hash functions instead of computed directly
		  from sorted runs."} 


@article{w:dyn,
	Author = "Eugene Wong",
	Title = "Dynamic Rematerialization: Processing Distributed
	         Queries Using Redundant Data",
	Journal = "IEEE Transactions on Software Engineering",
	Volume = 9,
	Number = 3,
	Pages = "228--232",
	Month = "May",
	Year = 1983,
	Annote = "More a framework for discussing algorithms than
	          an algorithm.  The computation is described as
		  a series of transitions, where transitions occur
		  due to transferring data from one site to another
		  or by some local operation.  The cost of the
		  computation is the some of the costs of the
		  transmissions.  Then choosing a strategy is
		  reduced to an optimal control problem.  The
		  problem is specifying costs accurately."}

@incollection{yt:par,
	Author = "Y. Yamane and R. Take",
	Title = "Parallel Partition Sort for Database Machines",
	Booktitle = "Database Machines and Knowledge Base Machines",
	Editor = "Masaru Kitsuregawa and Hidehiko Tanaka",
	Publisher = "Kluwer Academic Publishers",
	Year = 1988,
	Pages = "117 -- 130",
	Annote = "Appears to be a previous discovery of Iyer's algorithm
		for finding an exact partition of a file,
		although it's hard to tell because it's presented at
		a high level.  The authors note that the algorithm
		is $O(p^2\log p)$, so it is no good for fine grained
		machines."}

@inproceedings{ykh:ter,
	Author = "Haruo Yokota and Hajime Kitakami and Akira Hattori",
	Title = "Term Indexing for Retrieval by Unification",
	Booktitle = "Proceedings of the Fifth International Conference
	             on Data Engineering",
	Month = "February",
	Year = 1989,
	Address = "Los Angeles, California",
	Annote = "The authors propose a mixture of hashing and trie
            indices for terms.  Terms in their system are stored
	    in LOSR (Level Order Sequential Representation).  Part
	    of their justification for the Trie structure is to
	    speed unification --- essentially the unification is
	    performed once for each shared portion of the trie.
	    In addition to the trie, terms are hashed on ``key''
	    attributes of the term relation.  The trie structure
	    is essentially used to handle collisions.  The authors
	    benchmark their system on a database of terms of
	    specified shapes, and compare it to Quintus Prolog,
	    so it seems they are comparing implementations as
	    well as indexing schemes."}

@inproceedings{ytrgecbbb:the,
	Author = "Michael Young and Avadis Tevanian and Richard Rashid
                  and David Golub and Jeffrey Eppinger and Jonathan 
                  Chew and William Bolosky and David Black and 
                  Robert Baron",
	Title = "The Duality of Memory and Communication in the 
                 Implementation of a Multiprocessor Operating 
                 System",
	Booktitle = "Proceedings of the Eleventh ACM Symposium
                     on Operating Systems Principles",
        Month = "November",
	Year = 1987,
        Address = "Austin, Texas",
	Pages = "63--76",
	Annote = "Good discussion of VM in multiprocessors"}

@inproceedings{zg:ana,
	Author = "{Hansjo\"rg} Zeller and Jim Gray",
	Title = "An Adaptive Hash Join Algorithm for Multiuser Envirnoments",
	Booktitle = "Proceedings of the Sixteenth International Conference
		on Very Large Data Bases",
	Month = "August",
	Year = 1990,
	Pages = "186--197",
	Annote = "Describes a hash join algorithm that is intended to perform
		well in situations where the amount of main memory available
		is unknown or even varies during the execution.  The main
		idea seems to be to assign multiple hash buckets to a single
		{\em cluster}, and then to carefully grow, shrink, and write
		out these clusters to try to produce reasonable I/O performance."}

@inbook( Arlauskas88,
    author  = "Ramune Arlauskas",
    title   = "{iPSC/2} {System}: A Second Generation Hypercube",
    booktitle = "Concurrent Supercomputing, the Second Generation",
    year    = 1988,
    publisher = "Intel Corporation",
    pages   = "9--13"
    )

@inbook( Close88,
    author  = "Paul Close",
    title   = "The {iPSC/2 Node} Architecture",
    booktitle = "Concurrent Supercomputing, the Second Generation",
    year    = 1988,
    publisher = "Intel Corporation",
    pages   = "43--50"
    )

@inbook( Nugent88,
    author  = "Steven F. Nugent",
    title   = "The {iPSC/2} Direct-Connect Communications Technology",
    booktitle = "Concurrent Supercomputing, the Second Generation",
    year    = 1988,
    publisher = "Intel Corporation",
    pages   = "59--68"
    )


@article{hz:ash,
	Author = "M. F. Hornick and S. B. Zdonick",
	Title = "A Shared, Segmented Memory System for an 
                 Object-Oriented Database",
	Journal = "ACM Transactions on Office Information Systems",
	Volume = 5,
	Number = 1, 
	Year = 1987,
	Annote = "Covers such things as segment-at-a-time
                  transfer and object-level locking and their
                  interactions.  Acknowledges that clustering
                  is important, and suggests using runtime
                  statistics for reorganization."}

@inproceedings{bk:rtree,
	Author = "N. Beckmann and H.P. Kriegel and R. Schneider,
		  and B. Seeger",
	Title = "The $R^{*}$---Tree: An Efficient and Robust Access
		 Method for Points and Rectangles",
	Booktitle = "Proc. ACM SIGMOD Int. Conf. on Management of Data",
        Month = "June",
	Year = 1990,
        Address = "Atlantic City, NJ",
	Pages = "322-331",
	 }

@inbook( Kumar94,
    author  = "V. Kumar, Ed.",
    title   = "Concurrency Control and Recovery Methods for B+-Tree Indexes: ARIES/KVL and ARIES/IM",
    booktitle = "Performance of Concurrency Control Mechanism in Centralized Database Systems",
    year    = 1994,
    publisher = "Prentice-Hall",
    )

@techreport{m:kvl,
	Author = "C. Mohan",
    title   = "Concurrency Control and Recovery Methods for B+-Tree Indexes: ARIES/KVL and ARIES/IM",
	Institution = "IBM Almaden Research Center",
	Year = 1994,	
	Month = "March",
	Number = "RJ 9715 (84347)",
	Annote = "To appear in book by V. Kumar: Performance of Concurrency Control Mechanism in Centralized Database Systems"
	 }
